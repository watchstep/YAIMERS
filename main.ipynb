{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycaret'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     36\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpycaret\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycaret\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     41\u001b[0m pd\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mmax_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycaret'"
     ]
    }
   ],
   "source": [
    "############################# IMPORT LIBRARY  #################################\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import pickle\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# https://contrib.scikit-learn.org/category_encoders/index.html\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.feature_selection import RFE, SelectFromModel, SelectKBest, f_classif, chi2\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_auc_score, precision_score, recall_score\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pycaret\n",
    "from pycaret.classification import *\n",
    "\n",
    "pd.options.display.max_columns = 200\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################   CONFIG  #######################\n",
    "parser = argparse.ArgumentParser(description='Anomaly Detection')\n",
    "\n",
    "parser.add_argument('--data_path', type=str, default='./data')\n",
    "parser.add_argument('--seed',type=int, default=110)\n",
    "\n",
    "parser.add_argument('--model', type=str, default='cat')\n",
    "\n",
    "parser.add_argument('-en', '--encoder', type=str, default='le')\n",
    "parser.add_argument('-s', '--scaler', type=str, default='mms')\n",
    "\n",
    "downsample_options = {1:\"nearmiss\", 2:\"cluster\", 3:\"allknn\", 4:\"oneside\", 5:\"tomek\"}\n",
    "parser.add_argument('-ds', '--downsampling', type=int, default=5) # TOMEK\n",
    "\n",
    "upsample_options = {1: \"random\", 2:\"smote\", 3:\"adasyn\", 4:\"smotenc\", 5:\"smoten\", 6:\"borderline\", 7:\"kmeans\", 8:\"svm\"}\n",
    "parser.add_argument('-us', '--upsampling', type=int, default=3) # SMOTEE - NC\n",
    "\n",
    "parser.add_argument('--fs_mode', type=bool, default=False, help='feature selection T:auto F:manual')\n",
    "parser.add_argument('--estimator', type=str, default='extra', help=\"using for feature selection\")\n",
    "parser.add_argument('--selector', type=str, default='sfm', help='auto feature selector')\n",
    "\n",
    "parser.add_argument('--check_all', type=bool, default=False)\n",
    "parser.add_argument('--tune_mode', type=bool, default=True, help='optuna tuning')\n",
    "\n",
    "config = parser.parse_args([])\n",
    "\n",
    "exp_config = f\"{config.encoder}_{config.scaler}_{downsample_options[config.downsampling]}_{upsample_options[config.upsampling]}\"\n",
    "\n",
    "random.seed(config.seed)\n",
    "np.random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################   LOAD DATA  #######################\n",
    "df_tr = pd.read_csv(os.path.join(config.data_path, \"train_v2.csv\"))\n",
    "df_te = pd.read_csv(os.path.join(config.data_path, \"test_v2.csv\"))\n",
    "df_list = [df_tr, df_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################  FEATURE HANDLING  ###########################\n",
    "## CATEGORICAL FEATURES\n",
    "cat_features = [\"Equipment_Dam\",\n",
    "                \"Equipment_Fill1\",\n",
    "                \"Equipment_Fill2\",\n",
    "                \"Model.Suffix\",\n",
    "                \"Workorder Category\",\n",
    "                \"Chamber Temp. Judge Value_AutoClave\"]\n",
    "\n",
    "## BINNING FEATURES\n",
    "# bins_features = df_tr.columns[df_tr.columns.str.contains(r\".*Bins.*\")].tolist()\n",
    "# Bins 열 만드는 데 사용된 열\n",
    "from_bins_features = [re.sub(r'\\s*Bins\\s*', '', f).strip() for f in bins_features]\n",
    "\n",
    "# cat_features.extend(bins_features)\n",
    "\n",
    "for df in df_list:\n",
    "    df[cat_features] = df[cat_features].astype(\"category\")\n",
    "\n",
    "## NUMERICAL FEATURES\n",
    "num_features = df_tr.select_dtypes(exclude=[\"category\"]).columns.to_list()\n",
    "num_features.remove(\"target\")\n",
    "\n",
    "## ALL FEATURES\n",
    "all_features = num_features + cat_features\n",
    "\n",
    "## TARGET ENCODING\n",
    "df_tr[\"target\"] = df_tr[\"target\"].map({\"Normal\": 0, \"AbNormal\": 1})\n",
    "\n",
    "## DATA SPLITTING \n",
    "X_tr, y_tr = df_tr.drop(\"target\", axis=1), df_tr[\"target\"]\n",
    "X_te = df_te.drop(\"Set ID\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = X_tr[all_features]\n",
    "X_te = X_te[all_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################  FEATURE ENCODING/SCALING ###########################\n",
    "## ENCODING\n",
    "if config.encoder == \"le\":\n",
    "    le = LabelEncoder()\n",
    "    for cat_feature in cat_features:\n",
    "        X_tr[cat_feature] = le.fit_transform(X_tr[cat_feature])\n",
    "        X_te[cat_feature] = le.transform(X_te[cat_feature])\n",
    "        \n",
    "elif config.encoder == \"js\":\n",
    "    js = ce.JamesSteinEncoder(cols=cat_features)\n",
    "    \n",
    "    X_tr = js.fit_transform(X_tr, y_tr)\n",
    "    X_te = js.transform(X_te)\n",
    "    \n",
    "elif config.encoder == \"woe\":\n",
    "    woe = ce.WOEEncoder(cols=cat_features)\n",
    "    \n",
    "    X_tr = woe.fit_transform(X_tr, y_tr)\n",
    "    X_te = woe.transform(X_te)\n",
    "    \n",
    "elif config.encoder == \"ohe\": \n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    \n",
    "    encoded_tr = ohe.fit_transform(X_tr[cat_features])\n",
    "    encoded_df_tr = pd.DataFrame(encoded_tr, columns=ohe.get_feature_names_out())\n",
    "    X_tr = pd.concat([X_tr[num_features], encoded_df_tr], axis=1)\n",
    "    \n",
    "    encoded_te = ohe.transform(X_te[cat_features])\n",
    "    encoded_df_te = pd.DataFrame(encoded_te, columns=ohe.get_feature_names_out())\n",
    "    X_te = pd.concat([X_te[num_features], encoded_df_te], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SCALING\n",
    "if config.scaler == \"mms\":\n",
    "    mms = MinMaxScaler()\n",
    "    X_tr[num_features] = mms.fit_transform(X_tr[num_features])\n",
    "    X_te[num_features] = mms.transform(X_te[num_features])\n",
    "    \n",
    "elif config.scaler == \"ss\":\n",
    "    ss = StandardScaler()\n",
    "    X_tr[num_features] = ss.fit_transform(X_tr[num_features])\n",
    "    X_te[num_features] = ss.transform(X_te[num_features])\n",
    "    \n",
    "elif config.scaler == \"qt\":\n",
    "    qt = QuantileTransformer(random_state=config.seed, output_distribution='normal', n_quantiles=min(100, len(X_tr) // 5)) # n_quantiles = 1000\n",
    "    \n",
    "    X_tr[num_features] = qt.fit_transform(X_tr[num_features])\n",
    "    X_te[num_features] = qt.transform(X_te[num_features])\n",
    "\n",
    "elif config.scaler == \"pt\":\n",
    "    pts = PowerTransformer(method='yeo-johnson')\n",
    "    \n",
    "    X_tr[num_features] = pts.fit_transform(X_tr[num_features])\n",
    "    X_te[num_features] = pts.transform(X_te[num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWN SAMPLING\n",
      "=============\n",
      "Original dataset shape Counter({0: 36197, 1: 2210})\n",
      "Resampled dataset shape Counter({0: 35138, 1: 2210})\n",
      "UP SAMPLNG\n",
      "==========\n",
      "Original dataset shape Counter({0: 35138, 1: 2210})\n",
      "Resampled dataset shape Counter({0: 35138, 1: 34584})\n"
     ]
    }
   ],
   "source": [
    "#################################  DOWN SAMPLING  ###############################\n",
    "downsampled_df_tr = resampling.downsample(X_tr, y_tr, method=downsample_options[config.downsampling], random_seed=config.seed)\n",
    "\n",
    "#################################  UP SAMPLING  ###############################\n",
    "cat_idx = [downsampled_df_tr.columns.get_loc(col) for col in cat_features]\n",
    "# cat_idx = [X_tr.columns.get_loc(col) for col in cat_features]\n",
    "X_tr = downsampled_df_tr.drop(\"target\", axis=1)\n",
    "y_tr = downsampled_df_tr[\"target\"]\n",
    "\n",
    "upsampled_df_tr = resampling.upsample(X_tr, y_tr, cat_idx=cat_idx, method=upsample_options[config.upsampling], random_seed=config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RESAMPLED DATA\n",
    "# X_tr = downsampled_df_tr.drop(\"target\", axis=1)\n",
    "# y_tr = downsampled_df_tr[\"target\"]\n",
    "\n",
    "X_tr = upsampled_df_tr.drop(\"target\", axis=1)\n",
    "y_tr = upsampled_df_tr[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ MODEL ############### \n",
    "classifiers = {\n",
    "    \"cat\": CatBoostClassifier(random_state=config.seed, auto_class_weights=\"Balanced\"),\n",
    "    \"lgbm\": LGBMClassifier(random_state=config.seed,),\n",
    "    \"xgb\": XGBClassifier(random_state=config.seed, eval_metric='auc', objective=\"binary:logistic\"),\n",
    "    \"ada\": AdaBoostClassifier(random_state=config.seed),\n",
    "    \"rfc\": RandomForestClassifier(random_state=config.seed, class_weight='balanced'),\n",
    "    \"lr\": LogisticRegression(random_state=config.seed),\n",
    "    \"extra\": ExtraTreesClassifier(random_state=config.seed)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################  FEATURE SELECTION  ############################\n",
    "if config.fs_mode:\n",
    "    estimator = classifiers[config.estimator]\n",
    "    estimator.fit(X_tr, y_tr)\n",
    "    \n",
    "    selectors = {\n",
    "        'rfe': RFE(estimator=estimator, n_features_to_select=50),\n",
    "        'sfm': SelectFromModel(estimator=estimator, threshold=\"mean\"),\n",
    "        'kbest': SelectKBest(score_func=f_classif,),\n",
    "    }\n",
    "    \n",
    "    selector = selectors[config.selector]\n",
    "    \n",
    "    X_tr_selec = selector.fit_transform(X_tr, y_tr)\n",
    "    X_te_selec = selector.transform(X_te)\n",
    "    \n",
    "else:\n",
    "    # 기존 열 대신 Bins 열 사용\n",
    "    selected_features = [feature for feature in all_features if feature not in from_bins_features]\n",
    "    \n",
    "    X_tr_selec = X_tr[selected_features]\n",
    "    X_te_selec = X_te[selected_features]\n",
    "    \n",
    "print(\"FEATRUE SELECTION\")\n",
    "print(\"Before \", X_tr.shape)\n",
    "print(\"After \", X_tr_selec.shape, end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################  EVALUATION  ############################\n",
    "stk = StratifiedKFold(n_splits=10, random_state=config.seed, shuffle=True)\n",
    "rstk = RepeatedStratifiedKFold(n_splits=10, random_state=config.seed)\n",
    "\n",
    "if config.check_all:\n",
    "    classifiers_lst = list(classifiers.values())\n",
    "    \n",
    "    score_df = pd.DataFrame(columns=classifiers.keys())\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        scores = cross_val_score(clf, X_tr_selec, y_tr, scoring=\"f1\", cv=stk)\n",
    "        score_df[clf_name] = scores.mean()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'f1_weighted', 'roc_auc']\n",
    "    score_df = pd.DataFrame(columns=metrics)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        scores = cross_val_score(classifiers[\"cat\"], X_tr_selec, y_tr, scoring=metric, cv=stk)\n",
    "        score_df[metric] = scores.mean()\n",
    "    \n",
    "print(\"MODEL CHECK\")\n",
    "print(score_df, end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.0001, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_uniform('colsample_bylevel', 0.5, 1.0),\n",
    "        'od_type': trial.suggest_categorical(\"od_type\", [\"IncToDec\", \"Iter\"]),\n",
    "        'od_wait': trial.suggest_int(\"od_wait\", 10, 50),\n",
    "    }\n",
    "\n",
    "    cat_clf = CatBoostClassifier(**params, random_state=config.seed, auto_class_weights=\"Balanced\",) # eval_metric=\"TotalF1\"\n",
    "    \n",
    "    stk = StratifiedKFold(n_splits=10, random_state=config.seed, shuffle=True)\n",
    "    f1_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (tr_idx, val_idx) in enumerate(stk.split(X_tr_selec, y_tr)):\n",
    "        X_tr_fold, X_val_fold = X_tr.iloc[tr_idx], X_tr.iloc[val_idx]\n",
    "        y_tr_fold, y_val_fold = y_tr.iloc[tr_idx], y_tr.iloc[val_idx]\n",
    "        \n",
    "        cat_clf.fit(X_tr_fold, y_tr_fold, eval_set=[(X_val_fold, y_val_fold)], early_stopping_rounds=50, verbose=False)\n",
    "        y_pred_fold = cat_clf.predict(X_val_fold)\n",
    "        f1_scores[idx] = f1_score(y_val_fold, y_pred_fold)\n",
    "\n",
    "    return np.mean(f1_scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_objective(trial, X_tr, y_tr):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 1000),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
    "    }\n",
    "    \n",
    "    lgbm_clf = LGBMClassifier(**params, random_state=config.seed)\n",
    "    \n",
    "    stk = StratifiedKFold(n_splits=10, random_state=config.seed, shuffle=True)\n",
    "    f1_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (tr_idx, val_idx) in enumerate(stk.split(X_tr, y_tr)):\n",
    "        X_tr_fold, X_val_fold = X_tr.iloc[tr_idx], X_tr.iloc[val_idx]\n",
    "        y_tr_fold, y_val_fold = y_tr.iloc[tr_idx], y_tr.iloc[val_idx]\n",
    "        \n",
    "        lgbm_clf.fit(X_tr_fold, y_tr_fold, early_stopping_rounds=50, verbose=False)\n",
    "        y_pred_fold = lgbm_clf.predict(X_val_fold)\n",
    "        f1_scores[idx] = f1_score(y_val_fold, y_pred_fold)\n",
    "\n",
    "    return np.mean(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.0001, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),  # colsample_bylevel -> colsample_bytree\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),  # XGBoost의 중요한 파라미터 중 하나\n",
    "        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),  # 트리의 분할을 조정하는 파라미터\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),  # L1 정규화 항\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),  # L2 정규화 항\n",
    "    }\n",
    "\n",
    "    xgb_clf = XGBClassifier(**params, random_state=config.seed, use_label_encoder=False,\n",
    "                            eval_metric='auc',\n",
    "                            early_stopping_rounds=50, \n",
    "                            objective = \"binary:logistic\",)\n",
    "    \n",
    "    stk = StratifiedKFold(n_splits=10, random_state=config.seed, shuffle=True)\n",
    "    f1_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (tr_idx, val_idx) in enumerate(stk.split(X_tr_selec, y_tr)):\n",
    "        X_tr_fold, X_val_fold = X_tr.iloc[tr_idx], X_tr.iloc[val_idx]\n",
    "        y_tr_fold, y_val_fold = y_tr.iloc[tr_idx], y_tr.iloc[val_idx]\n",
    "        \n",
    "        xgb_clf.fit(X_tr_fold, y_tr_fold, eval_set=[(X_val_fold, y_val_fold)], early_stopping_rounds=50, verbose=False)\n",
    "        y_pred_fold = xgb_clf.predict(X_val_fold)\n",
    "        f1_scores[idx] = f1_score(y_val_fold, y_pred_fold)\n",
    "\n",
    "    return np.mean(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = logging.getLogger()\n",
    "    \n",
    "# logger.setLevel(logging.INFO)\n",
    "# logger.addHandler(logging.FileHandler(f\"./log/{config.model}_optuna.log\", mode=\"w\"))\n",
    "    \n",
    "# optuna.logging.enable_propagation() \n",
    "# optuna.logging.disable_default_handler()\n",
    "        \n",
    "sampler = TPESampler(seed=config.seed)\n",
    "pruner = optuna.pruners.MedianPruner()\n",
    "\n",
    "if config.tune_mode and config.model == \"cat\":\n",
    "    \n",
    "    cat_study = optuna.create_study(study_name=\"cat\", direction='maximize', sampler=sampler, pruner=pruner)\n",
    "    cat_study.optimize(catboost_objective, n_trials=15)\n",
    "    \n",
    "    cat_best_params = cat_study.best_params\n",
    "    cat_best_score = cat_study.best_value\n",
    "    \n",
    "    print(\"CatBoost Best Hyperparams: \", cat_best_params)\n",
    "    print(\"CatBoost Best F1 Score: \", cat_best_score, end='\\n')\n",
    "    \n",
    "    final_clf = CatBoostClassifier(**cat_best_params, random_state=config.seed, auto_class_weights=\"Balanced\",)\n",
    "    \n",
    "elif config.tune_mode and config.model == \"lgbm\":\n",
    "    \n",
    "    lgbm_study = optuna.create_study(study_name=\"lgbm\", direction='maximize', sampler=sampler, pruner=pruner)\n",
    "    lgbm_study.optimize(lgbm_objective(X_tr_selec, y_tr), n_trials=15)\n",
    "    \n",
    "    lgbm_best_params = lgbm_study.best_params\n",
    "    lgbm_best_score= lgbm_study.best_value\n",
    "    \n",
    "    print(\"LGBM Best Hyperparams: \",lgbm_best_params)\n",
    "    print(\"LGBM Best F1 Score: \", lgbm_best_score, end='\\n')\n",
    "    \n",
    "    final_clf = LGBMClassifier(**lgbm_best_params, random_state=config.seed,)\n",
    "\n",
    "elif config.tune_mode and config.model == \"xgb\":\n",
    "    \n",
    "    xgb_study = optuna.create_study(study_name=\"xgb\", direction='maximize', sampler=sampler, pruner=pruner)\n",
    "    xgb_study.optimize(xgboost_objective, n_trials=15)\n",
    "    \n",
    "    xgb_best_params = xgb_study.best_params\n",
    "    xgb_best_score = xgb_study.best_value\n",
    "    print(\"XGBoost Best Hyperparams: \", xgb_best_params)\n",
    "    print(\"XGBoost Best F1 Score: \", xgb_best_score)\n",
    "    \n",
    "else:\n",
    "    final_clf = classifiers[config.model]\n",
    "\n",
    "# with open(f\"./log/{config.model}_optuna.log\") as f:\n",
    "#     assert f.readline().startswith(\"A new study created\")\n",
    "#     assert f.readline() == \"Start optimization.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study.visualize()\n",
    "# study.trials_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#####################     SUBMISSION   #########################\n",
    "################################################################\n",
    "final_clf.fit(X_tr_selec, y_tr, ) # use_best_model=True\n",
    "final_preds = final_clf.predict(X_te_selec)\n",
    "\n",
    "df_sub = pd.read_csv(os.path.join(config.data_path, \"submission.csv\"))\n",
    "df_sub[\"target\"] = final_preds\n",
    "df_sub[\"target\"] = df_sub[\"target\"].map({0 : \"Normal\", 1 : \"AbNormal\"})\n",
    "\n",
    "print('=============================')\n",
    "print(df_sub[\"target\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_date = datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "\n",
    "# pickle.dump(final_clf, open(f\"{config.model}_{curr_date}.pkl\", \"wb\"))\n",
    "# final_clf = pickle.load(open(\".pkl\", \"rb\"))\n",
    "df_sub.to_csv(os.path.join(config.data_path, f\"submission_{curr_date}_{exp_config}.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
