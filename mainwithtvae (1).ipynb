{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# IMPORT LIBRARY  #################################\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import pickle\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# https://contrib.scikit-learn.org/category_encoders/index.html\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.feature_selection import RFE, SelectFromModel, SelectKBest, f_classif, chi2\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_auc_score, precision_score, recall_score\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler, NSGAIISampler, NSGAIIISampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################   CONFIG  #######################\n",
    "parser = argparse.ArgumentParser(description='Anomaly Detection')\n",
    "\n",
    "parser.add_argument('--data_path', type=str, default='./Data')\n",
    "parser.add_argument('--seed',type=int, default=881)\n",
    "\n",
    "parser.add_argument('--model', type=str, default='cat')\n",
    "\n",
    "parser.add_argument('-en', '--encoder', type=str, default='js')\n",
    "parser.add_argument('-s', '--scaler', type=str, default='qt')\n",
    "\n",
    "downsample_options = {1:\"nearmiss\", 2:\"cluster\", 3:\"allknn\", 4:\"oneside\", 5:\"tomek\"}\n",
    "parser.add_argument('-ds', '--downsampling', type=int, default=4) # TOMEK\n",
    "\n",
    "upsample_options = {1: \"random\", 2:\"smote\", 3:\"adasyn\", 4:\"smotenc\", 5:\"smoten\", 6:\"borderline\", 7:\"kmeans\", 8:\"svm\", 9: \"ctgan\", 10: \"tvae\"}\n",
    "parser.add_argument('-us', '--upsampling', type=int, default=2) # SVMSMOTE\n",
    "\n",
    "####### 추가 augmentation으로 CTGAN을 사용하는 경우 #############\n",
    "augmentation_options = {1: \"ctgan\", 2: \"tvae\"}\n",
    "parser.add_argument('-ag', '--augmentation', type=int, default=2)\n",
    "\n",
    "parser.add_argument('--fs_mode', type=bool, default=True, help='feature selection T:auto F:manual')\n",
    "parser.add_argument('--estimator', type=str, default='cat', help=\"using for feature selection\")\n",
    "parser.add_argument('--selector', type=str, default='kbest', help='auto feature selector')\n",
    "\n",
    "parser.add_argument('--k', type=int, default=10, help='k fold split')\n",
    "parser.add_argument('--check_all', type=bool, default=True)\n",
    "parser.add_argument('--tune_mode', type=bool, default=True, help='optuna tuning')\n",
    "\n",
    "config = parser.parse_args([])\n",
    "\n",
    "exp_config = f\"{config.encoder}_{config.scaler}_{downsample_options[config.downsampling]}_{upsample_options[config.upsampling]}\"\n",
    "\n",
    "random.seed(config.seed)\n",
    "np.random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data_path='./Data', seed=881, model='cat', encoder='js', scaler='qt', downsampling=4, upsampling=2, augmentation=2, fs_mode=True, estimator='cat', selector='kbest', k=10, check_all=True, tune_mode=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Workorder'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/work/Aimers/hari/mainwithtvae.ipynb Cell 4\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://proxy1.aitrain.ktcloud.com:10412/home/work/Aimers/hari/mainwithtvae.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Workorder (test에 있는데, train에는 없는 경우가 있어 그냥 제외)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://proxy1.aitrain.ktcloud.com:10412/home/work/Aimers/hari/mainwithtvae.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# 대신 Workorder Categeory 사용\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://proxy1.aitrain.ktcloud.com:10412/home/work/Aimers/hari/mainwithtvae.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m df_list:\n\u001b[0;32m----> <a href='vscode-notebook-cell://proxy1.aitrain.ktcloud.com:10412/home/work/Aimers/hari/mainwithtvae.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     df\u001b[39m.\u001b[39;49mdrop([\u001b[39m\"\u001b[39;49m\u001b[39mWorkorder\u001b[39;49m\u001b[39m\"\u001b[39;49m], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/prevenotics/miniconda3/envs/hrenv/lib/python3.12/site-packages/pandas/core/frame.py:5344\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[1;32m   5197\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   5198\u001b[0m     labels: IndexLabel \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5205\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   5206\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5208\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5209\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5342\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5343\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5344\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[1;32m   5345\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   5346\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   5347\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   5348\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   5349\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   5350\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[1;32m   5351\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   5352\u001b[0m     )\n",
      "File \u001b[0;32m~/prevenotics/miniconda3/envs/hrenv/lib/python3.12/site-packages/pandas/core/generic.py:4711\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4709\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   4710\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 4711\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4713\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[1;32m   4714\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/prevenotics/miniconda3/envs/hrenv/lib/python3.12/site-packages/pandas/core/generic.py:4753\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4751\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m   4752\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4753\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4754\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4756\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/prevenotics/miniconda3/envs/hrenv/lib/python3.12/site-packages/pandas/core/indexes/base.py:7000\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6998\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[1;32m   6999\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 7000\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabels[mask]\u001b[39m.\u001b[39mtolist()\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   7001\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[1;32m   7002\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Workorder'] not found in axis\""
     ]
    }
   ],
   "source": [
    "#######################   LOAD DATA  #######################\n",
    "df_tr = pd.read_csv(os.path.join(config.data_path, \"train_v2.csv\"))\n",
    "df_te = pd.read_csv(os.path.join(config.data_path, \"test_v2.csv\"))\n",
    "df_list = [df_tr, df_te]\n",
    "\n",
    "# Workorder (test에 있는데, train에는 없는 경우가 있어 그냥 제외)\n",
    "# 대신 Workorder Categeory 사용\n",
    "for df in df_list:\n",
    "    df.drop([\"Workorder\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################  FEATURE HANDLING  ###########################\n",
    "## CATEGORICAL FEATURES\n",
    "cat_features = [\"Equipment_Dam\",\n",
    "                \"Equipment_Fill1\",\n",
    "                \"Equipment_Fill2\",\n",
    "                \"Model.Suffix\",\n",
    "                \"Workorder Category\",\n",
    "                \"Chamber Temp. Judge Value_AutoClave\"]\n",
    "\n",
    "## BINNING FEATURES\n",
    "bins_features = df_tr.columns[df_tr.columns.str.contains(r\".*Bins.*\")].tolist()\n",
    "# Bins 열 만드는 데 사용된 열\n",
    "from_bins_features = [re.sub(r'\\s*Bins\\s*', '', f).strip() for f in bins_features]\n",
    "\n",
    "cat_features.extend(bins_features)\n",
    "\n",
    "for df in df_list:\n",
    "    df[cat_features] = df[cat_features].astype(\"category\")\n",
    "\n",
    "## NUMERICAL FEATURES\n",
    "num_features = df_tr.select_dtypes(exclude=[\"category\"]).columns.to_list()\n",
    "num_features.remove(\"target\")\n",
    "\n",
    "## ALL FEATURES\n",
    "all_features = num_features + cat_features\n",
    "\n",
    "## TARGET ENCODING\n",
    "df_tr[\"target\"] = df_tr[\"target\"].map({\"Normal\": 0, \"AbNormal\": 1})\n",
    "\n",
    "## DATA SPLITTING \n",
    "X_tr, y_tr = df_tr.drop(\"target\", axis=1), df_tr[\"target\"]\n",
    "X_te = df_te.drop(\"Set ID\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################  FEATURE ENCODING/SCALING ###########################\n",
    "## ENCODING\n",
    "if config.encoder == \"le\":\n",
    "    le = LabelEncoder()\n",
    "    for cat_feature in cat_features:\n",
    "        X_tr[cat_feature] = le.fit_transform(X_tr[cat_feature])\n",
    "        X_te[cat_feature] = le.transform(X_te[cat_feature])\n",
    "        \n",
    "elif config.encoder == \"js\":\n",
    "    js = ce.JamesSteinEncoder(cols=cat_features)\n",
    "    \n",
    "    X_tr = js.fit_transform(X_tr, y_tr)\n",
    "    X_te = js.transform(X_te)\n",
    "    \n",
    "elif config.encoder == \"woe\":\n",
    "    woe = ce.WOEEncoder(cols=cat_features)\n",
    "    \n",
    "    X_tr = woe.fit_transform(X_tr, y_tr)\n",
    "    X_te = woe.transform(X_te)\n",
    "    \n",
    "elif config.encoder == \"ohe\":\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "    \n",
    "    X_tr[cat_features] = ohe.fit_transform(X_tr[cat_features])\n",
    "    X_te[cat_features] = ohe.transform(X_te[cat_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SCALING\n",
    "if config.scaler == \"mms\":\n",
    "    mms = MinMaxScaler()\n",
    "    X_tr[num_features] = mms.fit_transform(X_tr[num_features])\n",
    "    X_te[num_features] = mms.transform(X_te[num_features])\n",
    "    \n",
    "elif config.scaler == \"ss\":\n",
    "    ss = StandardScaler()\n",
    "    X_tr[num_features] = ss.fit_transform(X_tr[num_features])\n",
    "    X_te[num_features] = ss.transform(X_te[num_features])\n",
    "    \n",
    "elif config.scaler == \"qt\":\n",
    "    qt = QuantileTransformer(random_state=config.seed, output_distribution='normal', n_quantiles=min(100, len(X_tr) // 5)) # n_quantiles = 1000\n",
    "    \n",
    "    X_tr[num_features] = qt.fit_transform(X_tr[num_features])\n",
    "    X_te[num_features] = qt.transform(X_te[num_features])\n",
    "\n",
    "elif config.scaler == \"pt\":\n",
    "    pts = PowerTransformer(method='yeo-johnson')\n",
    "    \n",
    "    X_tr[num_features] = pts.fit_transform(X_tr[num_features])\n",
    "    X_te[num_features] = pts.transform(X_te[num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.utils import resample\n",
    "from imblearn.under_sampling import (NearMiss,\n",
    "                                     ClusterCentroids,\n",
    "                                     AllKNN,\n",
    "                                     OneSidedSelection,\n",
    "                                     TomekLinks)\n",
    "from imblearn.over_sampling import (RandomOverSampler,\n",
    "                                    SMOTE,\n",
    "                                    ADASYN,\n",
    "                                    SMOTENC,\n",
    "                                    SMOTEN,\n",
    "                                    BorderlineSMOTE,\n",
    "                                    KMeansSMOTE,\n",
    "                                    SVMSMOTE)\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Import libraries for CTGAN and VAE\n",
    "from ctgan import CTGAN, TVAE\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "def random_downsample(df, random_seed, sample_ratio=1.0):\n",
    "    df_normal = df[df[\"target\"] == 0] \n",
    "    df_abnormal = df[df[\"target\"] == 1]\n",
    "    \n",
    "    downsampled = resample(\n",
    "        df_normal,\n",
    "        replace=False,\n",
    "        n_samples=int(len(df_abnormal) * sample_ratio),\n",
    "        random_state=random_seed\n",
    "    )\n",
    "    \n",
    "    downsampled_df = pd.concat([df_abnormal, downsampled])\n",
    "    \n",
    "    return downsampled_df\n",
    "\n",
    "def downsample(X, y, method, random_seed):\n",
    "    # NearMiss\n",
    "    if method == \"nearmiss\":\n",
    "        # sampling_strategy=\"auto\"\n",
    "        nm = NearMiss(sampling_strategy=0.4)\n",
    "        X_downsampled, y_downsampled = nm.fit_resample(X, y)\n",
    "    # ClusterCentroids\n",
    "    elif method == \"cluster\":\n",
    "        cc = ClusterCentroids(random_state=random_seed)\n",
    "        X_downsampled, y_downsampled = cc.fit_resample(X, y)\n",
    "    # AllKNN\n",
    "    elif method == \"allknn\":\n",
    "        allknn = AllKNN()\n",
    "        X_downsampled, y_downsampled = allknn.fit_resample(X, y)\n",
    "    # OneSidedSelection\n",
    "    elif method == \"oneside\":\n",
    "        oss = OneSidedSelection(random_state=random_seed)\n",
    "        X_downsampled, y_downsampled = oss.fit_resample(X, y)\n",
    "    # Tomeklinks\n",
    "    elif method == \"tomek\":\n",
    "        tl = TomekLinks()\n",
    "        X_downsampled, y_downsampled = tl.fit_resample(X, y)\n",
    "    \n",
    "    X_downsampled_df= pd.DataFrame(X_downsampled, columns=X.columns)\n",
    "    y_downsampled_df = pd.Series(y_downsampled, name=\"target\") \n",
    "    downsampled_df = pd.concat([X_downsampled_df, y_downsampled_df], axis=1)\n",
    "    \n",
    "    print('DOWN SAMPLING')\n",
    "    print('=============')\n",
    "    print('Original dataset shape %s' % Counter(y))\n",
    "    print('Resampled dataset shape %s' % Counter(y_downsampled), end='\\n')\n",
    "    \n",
    "    return downsampled_df\n",
    "\n",
    "\n",
    "def upsample(X, y, cat_idx, method, random_seed):\n",
    "    \n",
    "    if method == \"random\":\n",
    "        ros = RandomOverSampler(random_state=random_seed)\n",
    "        X_upsampled, y_upsampled = ros.fit_resample(X, y)\n",
    "        \n",
    "    # SMOTE\n",
    "    elif method == \"smote\":\n",
    "        smote = SMOTE(random_state=random_seed)#, sampling_strategy = 0.6)\n",
    "        X_upsampled, y_upsampled = smote.fit_resample(X, y)\n",
    "        \n",
    "    # ADASYN\n",
    "    elif method == \"adasyn\":\n",
    "        adasyn = ADASYN(random_state=random_seed)\n",
    "        X_upsampled, y_upsampled = adasyn.fit_resample(X, y)\n",
    "        \n",
    "    # SMOTE-NC\n",
    "    elif method == \"smotenc\":\n",
    "        smotenc = SMOTENC(random_state=random_seed, sampling_strategy=\"auto\", categorical_features=cat_idx)\n",
    "        X_upsampled, y_upsampled = smotenc.fit_resample(X, y)\n",
    "        \n",
    "    elif method == \"smoten\":\n",
    "        smoten = SMOTEN(random_state=random_seed, sampling_strategy=\"auto\", k_neighbors=5)\n",
    "        X_upsampled, y_upsampled = smoten.fit_resample(X, y)\n",
    "        \n",
    "    elif method == \"borderline\":\n",
    "        borderline_smote = BorderlineSMOTE(random_state=random_seed)\n",
    "        X_upsampled, y_upsampled = borderline_smote.fit_resample(X, y)\n",
    "        \n",
    "    elif method == \"kmeans\":\n",
    "        kmeans_smote = KMeansSMOTE(random_state=random_seed, sampling_strategy=\"auto\", k_neighbors=5)\n",
    "        X_upsampled, y_upsampled = kmeans_smote.fit_resample(X, y)\n",
    "        \n",
    "    elif method == \"svm\":\n",
    "        svm_smote = SVMSMOTE(random_state=42)\n",
    "        X_upsampled, y_upsampled = svm_smote.fit_resample(X, y)\n",
    "    \n",
    "    elif method == \"ctgan\":\n",
    "        X = pd.DataFrame(X)\n",
    "        # Assuming cat_idx is the list of indices of categorical features\n",
    "        discrete_columns = X.columns[cat_idx]  # Get the names of categorical columns\n",
    "\n",
    "        # Fit the CTGAN model on the original data\n",
    "        ctgan = CTGAN(epochs=100)\n",
    "        ctgan.fit(X, discrete_columns=discrete_columns)\n",
    "\n",
    "        # Generate synthetic samples\n",
    "        synthetic_samples = ctgan.sample(X.shape[0])\n",
    "\n",
    "        # Combine original and synthetic samples\n",
    "        X_upsampled = pd.concat([X, synthetic_samples])\n",
    "        y_upsampled = pd.concat([y, pd.Series([1] * synthetic_samples.shape[0], name=\"target\")])\n",
    "        \n",
    "    elif method == \"tvae\":\n",
    "            if isinstance(X, np.ndarray):\n",
    "                X = pd.DataFrame(X)\n",
    "            discrete_columns = X.columns[cat_idx]\n",
    "            \n",
    "            tvae = TVAE(epochs=100)\n",
    "            tvae.fit(X, discrete_columns=discrete_columns)\n",
    "            synthetic_samples = tvae.sample(X.shape[0])\n",
    "            \n",
    "            X_upsampled = pd.concat([X, synthetic_samples])\n",
    "            y_upsampled = pd.concat([y, pd.Series([1] * synthetic_samples.shape[0], name=\"target\")])\n",
    "            \n",
    "\n",
    "    upsampled_df = pd.concat([X_upsampled, y_upsampled], axis=1)\n",
    "        \n",
    "    print('UP SAMPLNG')\n",
    "    print('==========')\n",
    "    print('Original dataset shape %s' % Counter(y))\n",
    "    print('Resampled dataset shape %s' % Counter(y_upsampled), end='\\n')\n",
    "    \n",
    "    return upsampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################  DOWN SAMPLING  ###############################\n",
    "downsampled_df_tr = downsample(X_tr, y_tr, method=downsample_options[config.downsampling], random_seed=config.seed)\n",
    "\n",
    "#################################  UP SAMPLING  ###############################\n",
    "cat_idx = [downsampled_df_tr.columns.get_loc(col) for col in cat_features]\n",
    "cat_idx = [X_tr.columns.get_loc(col) for col in cat_features]\n",
    "X_tr = downsampled_df_tr.drop(\"target\", axis=1)\n",
    "y_tr = downsampled_df_tr[\"target\"]\n",
    "\n",
    "upsampled_df_tr = upsample(X_tr, y_tr, cat_idx=cat_idx, method=upsample_options[config.upsampling], random_seed=config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RESAMPLED DATA\n",
    "# X_tr = downsampled_df_tr.drop(\"target\", axis=1)\n",
    "# y_tr = downsampled_df_tr[\"target\"]\n",
    "\n",
    "X_tr = upsampled_df_tr.drop(\"target\", axis=1)\n",
    "y_tr = upsampled_df_tr[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.to_csv(os.path.join(config.data_path, f\"train_smote.csv\"), index=False)\n",
    "y_tr.to_csv(os.path.join(config.data_path, f\"test_smote.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# 로그 설정\n",
    "logging.basicConfig(filename='augmentation.log', level=logging.INFO, \n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "def augmentation(X, y, method, cat_idx=cat_idx, random_seed, sample_size):\n",
    "    logging.info('AUGMENTATION STARTED')\n",
    "    logging.info('Original dataset shape %s' % Counter(y))\n",
    "    \n",
    "    # Ensure X is a DataFrame\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "    \n",
    "    # Assuming cat_idx is the list of indices of categorical features\n",
    "    discrete_columns = X.columns[cat_idx]  # Get the names of categorical columns\n",
    "\n",
    "    if method == \"ctgan\":\n",
    "        logging.info(\"Starting CTGAN augmentation...\")\n",
    "        # Fit the CTGAN model on the original data\n",
    "        ctgan = CTGAN(epochs=100)\n",
    "        ctgan.fit(X, discrete_columns=discrete_columns)\n",
    "\n",
    "        logging.info(f\"Generating {sample_size} synthetic samples using CTGAN...\")\n",
    "        # Generate the specified number of synthetic samples\n",
    "        synthetic_samples = ctgan.sample(sample_size)\n",
    "\n",
    "        logging.info(\"Combining original and synthetic samples...\")\n",
    "        # Combine original and synthetic samples\n",
    "        X_upsampled = pd.concat([X, synthetic_samples])\n",
    "        y_upsampled = pd.concat([y, pd.Series([1] * synthetic_samples.shape[0], name=\"target\")])\n",
    "        \n",
    "    elif method == \"tvae\":\n",
    "        logging.info(\"Starting TVAE augmentation...\")\n",
    "        # Fit the TVAE model on the original data\n",
    "        tvae = TVAE(epochs=100)\n",
    "        tvae.fit(X, discrete_columns=discrete_columns)\n",
    "\n",
    "        logging.info(f\"Generating {sample_size} synthetic samples using TVAE...\")\n",
    "        # Generate the specified number of synthetic samples\n",
    "        synthetic_samples = tvae.sample(sample_size)\n",
    "\n",
    "        logging.info(\"Combining original and synthetic samples...\")\n",
    "        # Combine original and synthetic samples\n",
    "        X_upsampled = pd.concat([X, synthetic_samples])\n",
    "        y_upsampled = pd.concat([y, pd.Series([1] * synthetic_samples.shape[0], name=\"target\")])\n",
    "    \n",
    "    augment_df = pd.concat([X_upsampled, y_upsampled], axis=1)\n",
    "    \n",
    "    logging.info('Resampled dataset shape %s' % Counter(y_upsampled))\n",
    "    logging.info(\"Augmentation completed successfully.\")\n",
    "    \n",
    "    return augment_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_size = X_tr.shape[0]//4\n",
    "augmentation_df = augmentation(X_tr, y_tr, method=augmentation_options[config.augmentation], cat_idx=cat_idx, random_seed=config.seed, sample_size = custom_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = augmentation_df.drop(\"target\", axis=1)\n",
    "y_tr = augmentation_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ MODEL ############### \n",
    "classifiers = {\n",
    "    \"cat\": CatBoostClassifier(random_state=config.seed, auto_class_weights=\"SqrtBalanced\"),\n",
    "    \"lgbm\": LGBMClassifier(random_state=config.seed,),\n",
    "    \"xgb\": XGBClassifier(random_state=config.seed, eval_metric='auc', objective=\"binary:logistic\"),\n",
    "    \"ada\": AdaBoostClassifier(random_state=config.seed),\n",
    "    \"rfc\": RandomForestClassifier(random_state=config.seed, class_weight='balanced'),\n",
    "    \"lr\": LogisticRegression(random_state=config.seed),\n",
    "    \"extra\": ExtraTreesClassifier(random_state=config.seed)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################  FEATURE SELECTION  ############################\n",
    "if config.fs_mode:\n",
    "    estimator = classifiers[config.estimator]\n",
    "    estimator.fit(X_tr, y_tr)\n",
    "    \n",
    "    selectors = {\n",
    "        'rfe': RFE(estimator=estimator, n_features_to_select=50),\n",
    "        'sfm': SelectFromModel(estimator=estimator, threshold=\"mean\"),\n",
    "        'kbest': SelectKBest(score_func=f_classif, k=10),\n",
    "    }\n",
    "    \n",
    "    selector = selectors[config.selector]\n",
    "    \n",
    "    # Fit the selector on the training data\n",
    "    selector.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Get the support mask of selected features\n",
    "    support_mask = selector.get_support()\n",
    "    \n",
    "    # Retain selected features from the original DataFrame and preserve feature names\n",
    "    X_tr_selec = X_tr.loc[:, support_mask]\n",
    "    X_te_selec = X_te.loc[:, support_mask]\n",
    "    \n",
    "else:\n",
    "    # 기존 열 대신 Bins 열 사용\n",
    "    selected_features = [feature for feature in all_features if feature not in from_bins_features]\n",
    "    \n",
    "    X_tr_selec = X_tr[selected_features]\n",
    "    X_te_selec = X_te[selected_features]\n",
    "    \n",
    "print(\"FEATRUE SELECTION\")\n",
    "print(\"Before \", X_tr.shape)\n",
    "print(\"After \", X_tr_selec.shape, end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_selec.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###############################  EVALUATION  ############################\n",
    "# stk = StratifiedKFold(n_splits=10, random_state=config.seed, shuffle=True)\n",
    "# rstk = RepeatedStratifiedKFold(n_splits=10, random_state=config.seed)\n",
    "\n",
    "# if config.check_all:\n",
    "#     classifiers_lst = list(classifiers.values())\n",
    "#     score_df = pd.DataFrame(columns=classifiers.keys())\n",
    "    \n",
    "#     for clf_name, clf in classifiers.items():\n",
    "#         scores = cross_val_score(clf, X_tr_selec, y_tr, scoring=\"f1\", cv=stk)\n",
    "#         print(scores)\n",
    "#         score_df.loc[0, clf_name] = scores.mean()\n",
    "#         print(score_df)\n",
    "    \n",
    "    \n",
    "# else:\n",
    "#     metrics = ['accuracy', 'precision', 'recall', 'f1', 'f1_weighted', 'roc_auc']\n",
    "#     score_df = pd.DataFrame(columns=metrics)\n",
    "    \n",
    "#     for metric in metrics:\n",
    "#         scores = cross_val_score(classifiers[\"cat\"], X_tr_selec, y_tr, scoring=metric, cv=stk)\n",
    "#         score_df[metric] = scores.mean()\n",
    "    \n",
    "# print(\"MODEL CHECK\")\n",
    "# print(score_df, end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.0001, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_uniform('colsample_bylevel', 0.5, 1.0),\n",
    "        'od_type': trial.suggest_categorical(\"od_type\", [\"IncToDec\", \"Iter\"]),\n",
    "        'od_wait': trial.suggest_int(\"od_wait\", 10, 50),\n",
    "    }\n",
    "\n",
    "    cat_clf = CatBoostClassifier(**params, random_state=config.seed, auto_class_weights=\"Balanced\",) # eval_metric=\"TotalF1\"\n",
    "    \n",
    "    stk = StratifiedKFold(n_splits=config.k, random_state=config.seed, shuffle=True)\n",
    "    f1_scores = np.empty(config.k)\n",
    "    \n",
    "    for idx, (tr_idx, val_idx) in enumerate(stk.split(X_tr_selec, y_tr)):\n",
    "        X_tr_fold, X_val_fold = X_tr.iloc[tr_idx], X_tr.iloc[val_idx]\n",
    "        y_tr_fold, y_val_fold = y_tr.iloc[tr_idx], y_tr.iloc[val_idx]\n",
    "        \n",
    "        cat_clf.fit(X_tr_fold, y_tr_fold, eval_set=[(X_val_fold, y_val_fold)], early_stopping_rounds=50, verbose=False)\n",
    "        y_pred_fold = cat_clf.predict(X_val_fold)\n",
    "        f1_scores[idx] = f1_score(y_val_fold, y_pred_fold)\n",
    "\n",
    "    return np.mean(f1_scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 1000),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
    "    }\n",
    "    \n",
    "    lgbm_clf = LGBMClassifier(**params, random_state=config.seed)\n",
    "    \n",
    "    stk = StratifiedKFold(n_splits=config.k, random_state=config.seed, shuffle=True)\n",
    "    f1_scores = np.empty(config.k)\n",
    "    \n",
    "    for idx, (tr_idx, val_idx) in enumerate(stk.split(X_tr_selec, y_tr)):\n",
    "        X_tr_fold, X_val_fold = X_tr_selec.iloc[tr_idx], X_tr_selec.iloc[val_idx]\n",
    "        y_tr_fold, y_val_fold = y_tr.iloc[tr_idx], y_tr.iloc[val_idx]\n",
    "        \n",
    "        lgbm_clf.fit(X_tr_fold, y_tr_fold)\n",
    "        y_pred_fold = lgbm_clf.predict(X_val_fold)\n",
    "        f1_scores[idx] = f1_score(y_val_fold, y_pred_fold)\n",
    "\n",
    "    return np.mean(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.0001, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),  # colsample_bylevel -> colsample_bytree\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),  # XGBoost의 중요한 파라미터 중 하나\n",
    "        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),  # 트리의 분할을 조정하는 파라미터\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),  # L1 정규화 항\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),  # L2 정규화 항\n",
    "    }\n",
    "\n",
    "    xgb_clf = XGBClassifier(**params, random_state=config.seed, use_label_encoder=False,\n",
    "                            eval_metric='auc',\n",
    "                            early_stopping_rounds=50, \n",
    "                            objective = \"binary:logistic\",)\n",
    "    \n",
    "    stk = StratifiedKFold(n_splits=config.k, random_state=config.seed, shuffle=True)\n",
    "    f1_scores = np.empty(config.k)\n",
    "    \n",
    "    for idx, (tr_idx, val_idx) in enumerate(stk.split(X_tr_selec, y_tr)):\n",
    "        X_tr_fold, X_val_fold = X_tr.iloc[tr_idx], X_tr.iloc[val_idx]\n",
    "        y_tr_fold, y_val_fold = y_tr.iloc[tr_idx], y_tr.iloc[val_idx]\n",
    "        \n",
    "        xgb_clf.fit(X_tr_fold, y_tr_fold, eval_set=[(X_val_fold, y_val_fold)], early_stopping_rounds=50, verbose=False)\n",
    "        y_pred_fold = xgb_clf.predict(X_val_fold)\n",
    "        f1_scores[idx] = f1_score(y_val_fold, y_pred_fold)\n",
    "\n",
    "    return np.mean(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = logging.getLogger()\n",
    "    \n",
    "# logger.setLevel(logging.INFO)\n",
    "# logger.addHandler(logging.FileHandler(f\"./log/{config.model}_optuna.log\", mode=\"w\"))\n",
    "    \n",
    "# optuna.logging.enable_propagation() \n",
    "# optuna.logging.disable_default_handler()\n",
    "        \n",
    "sampler = NSGAIISampler(seed=config.seed)\n",
    "pruner = optuna.pruners.HyperbandPruner()\n",
    "\n",
    "if config.tune_mode and config.model == \"cat\":\n",
    "    \n",
    "    cat_study = optuna.create_study(study_name=\"cat\", direction='maximize', sampler=sampler, pruner=pruner)\n",
    "    cat_study.optimize(catboost_objective, n_trials=10)\n",
    "    \n",
    "    cat_best_params = cat_study.best_params\n",
    "    cat_best_score = cat_study.best_value\n",
    "    \n",
    "    print(\"CatBoost Best Hyperparams: \", cat_best_params)\n",
    "    print(\"CatBoost Best F1 Score: \", cat_best_score, end='\\n')\n",
    "    \n",
    "    final_clf = CatBoostClassifier(**cat_best_params, random_state=config.seed, auto_class_weights=\"Balanced\",)\n",
    "    \n",
    "elif config.tune_mode and config.model == \"lgbm\":\n",
    "    \n",
    "    lgbm_study = optuna.create_study(study_name=\"lgbm\", direction='maximize', sampler=sampler, pruner=pruner)\n",
    "    lgbm_study.optimize(lgbm_objective, n_trials=15)\n",
    "    \n",
    "    lgbm_best_params = lgbm_study.best_params\n",
    "    lgbm_best_score= lgbm_study.best_value\n",
    "    \n",
    "    print(\"LGBM Best Hyperparams: \",lgbm_best_params)\n",
    "    print(\"LGBM Best F1 Score: \", lgbm_best_score, end='\\n')\n",
    "    \n",
    "    final_clf = LGBMClassifier(**lgbm_best_params, random_state=config.seed,)\n",
    "\n",
    "elif config.tune_mode and config.model == \"xgb\":\n",
    "    \n",
    "    xgb_study = optuna.create_study(study_name=\"xgb\", direction='maximize', sampler=sampler, pruner=pruner)\n",
    "    xgb_study.optimize(xgboost_objective, n_trials=15)\n",
    "    \n",
    "    xgb_best_params = xgb_study.best_params\n",
    "    xgb_best_score = xgb_study.best_value\n",
    "    print(\"XGBoost Best Hyperparams: \", xgb_best_params)\n",
    "    print(\"XGBoost Best F1 Score: \", xgb_best_score)\n",
    "    \n",
    "else:\n",
    "    final_clf = classifiers[config.model]\n",
    "\n",
    "# with open(f\"./log/{config.model}_optuna.log\") as f:\n",
    "#     assert f.readline().startswith(\"A new study created\")\n",
    "#     assert f.readline() == \"Start optimization.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_selec.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_study.visualize()\n",
    "#cat_study.trials_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#####################     SUBMISSION   #########################\n",
    "################################################################\n",
    "final_clf.fit(X_tr_selec, y_tr, ) # use_best_model=True\n",
    "final_preds = final_clf.predict(X_te_selec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(\"/home/work/Aimers/data/submission.csv\")\n",
    "df_sub[\"target\"] = final_preds\n",
    "df_sub[\"target\"] = df_sub[\"target\"].map({0 : \"Normal\", 1 : \"AbNormal\"})\n",
    "\n",
    "print('=============================')\n",
    "print(df_sub[\"target\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_date = datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "\n",
    "# pickle.dump(final_clf, open(f\"{config.model}_{curr_date}.pkl\", \"wb\"))\n",
    "# final_clf = pickle.load(open(\".pkl\", \"rb\"))\n",
    "df_sub.to_csv(os.path.join(config.data_path, f\"submission_{curr_date}_{exp_config}.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
