{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017e9265",
   "metadata": {},
   "source": [
    "# 제품 이상여부 판별 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdab431",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8341e8",
   "metadata": {},
   "source": [
    "### 필수 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a315cc58",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/work/Aimers/code.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://proxy2.aitrain.ktcloud.com:10403/home/work/Aimers/code.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimblearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39munder_sampling\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomUnderSampler\n\u001b[1;32m     <a href='vscode-notebook-cell://proxy2.aitrain.ktcloud.com:10403/home/work/Aimers/code.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://proxy2.aitrain.ktcloud.com:10403/home/work/Aimers/code.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcatboost\u001b[39;00m \u001b[39mimport\u001b[39;00m CatBoostClassifier\n\u001b[1;32m     <a href='vscode-notebook-cell://proxy2.aitrain.ktcloud.com:10403/home/work/Aimers/code.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimblearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39munder_sampling\u001b[39;00m \u001b[39mimport\u001b[39;00m TomekLinks, CondensedNearestNeighbour\n\u001b[1;32m     <a href='vscode-notebook-cell://proxy2.aitrain.ktcloud.com:10403/home/work/Aimers/code.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimblearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mover_sampling\u001b[39;00m \u001b[39mimport\u001b[39;00m ADASYN\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/site-packages/catboost/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     FeaturesData, EFstrType, EShapCalcType, EFeaturesSelectionAlgorithm, EFeaturesSelectionGrouping,\n\u001b[1;32m      3\u001b[0m     Pool, CatBoost, CatBoostClassifier, CatBoostRegressor, CatBoostRanker, CatBoostError, cv, sample_gaussian_process, train,\n\u001b[1;32m      4\u001b[0m     sum_models, _have_equal_features, to_regressor, to_classifier, to_ranker, MultiRegressionCustomMetric,\n\u001b[1;32m      5\u001b[0m     MultiRegressionCustomObjective, MultiTargetCustomMetric, MultiTargetCustomObjective\n\u001b[1;32m      6\u001b[0m )  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m VERSION \u001b[39mas\u001b[39;00m __version__  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m      8\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mFeaturesData\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mEFstrType\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mEShapCalcType\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mEFeaturesSelectionAlgorithm\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mEFeaturesSelectionGrouping\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mPool\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCatBoost\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCatBoostClassifier\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCatBoostRegressor\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCatBoostRanker\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCatboostError\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mMultiTargetCustomMetric\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMultiTargetCustomObjective\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     14\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/site-packages/catboost/core.py:45\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mplot_helpers\u001b[39;00m \u001b[39mimport\u001b[39;00m save_plot_file, try_plot_offline, OfflineMetricVisualizer\n\u001b[1;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _catboost\n\u001b[1;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m BuiltinMetric\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/site-packages/catboost/plot_helpers.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _catboost\n\u001b[1;32m      6\u001b[0m fspath \u001b[39m=\u001b[39m _catboost\u001b[39m.\u001b[39mfspath\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtry_plot_offline\u001b[39m(figs):\n",
      "File \u001b[0;32m_catboost.pyx:1\u001b[0m, in \u001b[0;36minit _catboost\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import KFold,train_test_split\n",
    "from tqdm import tqdm\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import random\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.under_sampling import TomekLinks, CondensedNearestNeighbour\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "random_state = 42  # Random state for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bdead2",
   "metadata": {},
   "source": [
    "### 필요한 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada37547-37dc-4c3a-893c-c8b32bf446fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/work/Aimers/code.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://proxy2.aitrain.ktcloud.com:10403/home/work/Aimers/code.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m X \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mMERGED.csv\u001b[39m\u001b[39m\"\u001b[39m, low_memory \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"MERGED.csv\", low_memory = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 이상치 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns: ['Dam dispensing_Equipment', 'Dam dispensing_Model.Suffix', 'Dam dispensing_Workorder', 'Dam dispensing_Collect Date', 'Dam dispensing_Collect Result', 'Dam dispensing_Collect Result.1', 'Dam dispensing_Collect Result.2', 'Dam dispensing_Collect Result.3', 'Dam dispensing_Collect Result.7', 'Dam dispensing_Collect Result.9', 'Dam dispensing_Collect Result.10', 'Dam dispensing_Collect Result.11', 'Dam dispensing_Collect Result.12', 'Dam dispensing_Collect Result.13', 'Dam dispensing_Collect Result.14', 'Dam dispensing_Collect Result.15', 'Dam dispensing_Collect Result.16', 'Dam dispensing_Collect Result.17', 'Dam dispensing_Collect Result.18', 'Dam dispensing_Collect Result.19', 'Dam dispensing_Collect Result.20', 'Dam dispensing_Collect Result.21', 'Dam dispensing_Collect Result.22', 'Dam dispensing_Collect Result.23', 'Dam dispensing_Collect Result.24', 'Dam dispensing_Collect Result.25', 'Dam dispensing_Collect Result.26', 'Dam dispensing_Collect Result.27', 'Dam dispensing_Collect Result.28', 'Dam dispensing_Collect Result.29', 'Dam dispensing_Collect Result.30', 'Dam dispensing_Collect Result.31', 'Dam dispensing_Collect Result.32', 'Dam dispensing_Collect Result.33', 'Dam dispensing_Collect Result.34', 'Dam dispensing_Collect Result.35', 'Dam dispensing_Collect Result.36', 'Dam dispensing_Collect Result.37', 'Dam dispensing_Collect Result.38', 'Dam dispensing_Collect Result.39', 'Dam dispensing_Collect Result.40', 'Dam dispensing_Collect Result.41', 'Dam dispensing_Collect Result.42', 'Dam dispensing_Collect Result.43', 'Dam dispensing_Collect Result.44', 'Dam dispensing_Collect Result.45', 'Dam dispensing_Collect Result.46', 'Dam dispensing_Collect Result.47', 'Dam dispensing_Collect Result.48', 'Dam dispensing_Collect Result.49', 'Dam dispensing_Collect Result.50', 'Dam dispensing_Collect Result.51', 'Dam dispensing_Collect Result.52', 'Dam dispensing_Collect Result.53', 'Dam dispensing_Collect Result.54', 'Dam dispensing_Collect Result.55', 'Dam dispensing_Collect Result.56', 'Dam dispensing_Collect Result.57', 'Dam dispensing_Collect Result.58', 'Dam dispensing_Collect Result.59', 'Dam dispensing_Collect Result.60', 'Dam dispensing_Collect Result.61', 'Dam dispensing_Collect Result.62', 'Dam dispensing_Collect Result.63', 'Dam dispensing_Collect Result.64', 'Dam dispensing_Collect Result.65', 'Dam dispensing_Collect Result.66', 'Dam dispensing_Collect Result.67', 'Dam dispensing_Collect Result.68', 'Dam dispensing_Collect Result.69', 'Auto clave_Model.Suffix', 'Auto clave_Workorder', 'Auto clave_LOT ID', 'Auto clave_Collect Date', 'Auto clave_Collect Result', 'Auto clave_Unit Time', 'Auto clave_Collect Result.1', 'Auto clave_Unit Time.1', 'Auto clave_Collect Result.2', 'Auto clave_Unit Time.2', 'Auto clave_Collect Result.3', 'Auto clave_Unit Time.3', 'Auto clave_Judge Value.3', 'Fill1 dispensing_Equipment', 'Fill1 dispensing_Model.Suffix', 'Fill1 dispensing_Workorder', 'Fill1 dispensing_LOT ID', 'Fill1 dispensing_Collect Date', 'Fill1 dispensing_Collect Result', 'Fill1 dispensing_Collect Result.1', 'Fill1 dispensing_Collect Result.2', 'Fill1 dispensing_Collect Result.3', 'Fill1 dispensing_Collect Result.4', 'Fill1 dispensing_Collect Result.5', 'Fill1 dispensing_Collect Result.6', 'Fill1 dispensing_Collect Result.7', 'Fill1 dispensing_Collect Result.8', 'Fill1 dispensing_Collect Result.9', 'Fill1 dispensing_Collect Result.10', 'Fill1 dispensing_Collect Result.11', 'Fill1 dispensing_Collect Result.12', 'Fill1 dispensing_Collect Result.13', 'Fill1 dispensing_Collect Result.14', 'Fill1 dispensing_Collect Result.15', 'Fill1 dispensing_Collect Result.16', 'Fill1 dispensing_Collect Result.17', 'Fill1 dispensing_Collect Result.18', 'Fill1 dispensing_Collect Result.19', 'Fill1 dispensing_Collect Result.20', 'Fill1 dispensing_Collect Result.21', 'Fill1 dispensing_Collect Result.22', 'Fill1 dispensing_Collect Result.23', 'Fill1 dispensing_Collect Result.24', 'Fill1 dispensing_Collect Result.25', 'Fill1 dispensing_Collect Result.26', 'Fill1 dispensing_Collect Result.27', 'Fill1 dispensing_Collect Result.28', 'Fill1 dispensing_Collect Result.29', 'Fill2 dispensing_Equipment', 'Fill2 dispensing_Model.Suffix', 'Fill2 dispensing_Workorder', 'Fill2 dispensing_LOT ID', 'Fill2 dispensing_Collect Date', 'Fill2 dispensing_Collect Result', 'Fill2 dispensing_Collect Result.1', 'Fill2 dispensing_Collect Result.3', 'Fill2 dispensing_Collect Result.5', 'Fill2 dispensing_Collect Result.7', 'Fill2 dispensing_Collect Result.8', 'Fill2 dispensing_Collect Result.17', 'Fill2 dispensing_Collect Result.18', 'Fill2 dispensing_Collect Result.19', 'Fill2 dispensing_Collect Result.20', 'Fill2 dispensing_Collect Result.21', 'Fill2 dispensing_Collect Result.22', 'Fill2 dispensing_Collect Result.23', 'Fill2 dispensing_Collect Result.24', 'Fill2 dispensing_Collect Result.25', 'Fill2 dispensing_Collect Result.26', 'Fill2 dispensing_Collect Result.27', 'Fill2 dispensing_Collect Result.28', 'Fill2 dispensing_Collect Result.29', 'Fill2 dispensing_Collect Result.30', 'Fill2 dispensing_Collect Result.31', 'Fill2 dispensing_Collect Result.32', 'Fill2 dispensing_Collect Result.33', 'Fill2 dispensing_Collect Result.34', 'Fill2 dispensing_Collect Result.35', 'Fill2 dispensing_Collect Result.36', 'Fill2 dispensing_Collect Result.37', 'Fill2 dispensing_Collect Result.38', 'Fill2 dispensing_Collect Result.39', 'target']\n",
      "Cleaned columns: ['Dam dispensing_Equipment', 'Dam dispensing_Model.Suffix', 'Dam dispensing_Workorder', 'Dam dispensing_Collect Date', 'Dam dispensing_Collect Result', 'Dam dispensing_Collect Result.1', 'Dam dispensing_Collect Result.2', 'Dam dispensing_Collect Result.3', 'Dam dispensing_Collect Result.7', 'Dam dispensing_Collect Result.9', 'Dam dispensing_Collect Result.10', 'Dam dispensing_Collect Result.11', 'Dam dispensing_Collect Result.12', 'Dam dispensing_Collect Result.13', 'Dam dispensing_Collect Result.14', 'Dam dispensing_Collect Result.15', 'Dam dispensing_Collect Result.16', 'Dam dispensing_Collect Result.17', 'Dam dispensing_Collect Result.18', 'Dam dispensing_Collect Result.19', 'Dam dispensing_Collect Result.20', 'Dam dispensing_Collect Result.21', 'Dam dispensing_Collect Result.22', 'Dam dispensing_Collect Result.23', 'Dam dispensing_Collect Result.24', 'Dam dispensing_Collect Result.25', 'Dam dispensing_Collect Result.26', 'Dam dispensing_Collect Result.27', 'Dam dispensing_Collect Result.28', 'Dam dispensing_Collect Result.29', 'Dam dispensing_Collect Result.30', 'Dam dispensing_Collect Result.31', 'Dam dispensing_Collect Result.32', 'Dam dispensing_Collect Result.33', 'Dam dispensing_Collect Result.34', 'Dam dispensing_Collect Result.35', 'Dam dispensing_Collect Result.36', 'Dam dispensing_Collect Result.37', 'Dam dispensing_Collect Result.38', 'Dam dispensing_Collect Result.39', 'Dam dispensing_Collect Result.40', 'Dam dispensing_Collect Result.41', 'Dam dispensing_Collect Result.42', 'Dam dispensing_Collect Result.43', 'Dam dispensing_Collect Result.44', 'Dam dispensing_Collect Result.45', 'Dam dispensing_Collect Result.46', 'Dam dispensing_Collect Result.47', 'Dam dispensing_Collect Result.48', 'Dam dispensing_Collect Result.49', 'Dam dispensing_Collect Result.50', 'Dam dispensing_Collect Result.51', 'Dam dispensing_Collect Result.52', 'Dam dispensing_Collect Result.53', 'Dam dispensing_Collect Result.54', 'Dam dispensing_Collect Result.55', 'Dam dispensing_Collect Result.56', 'Dam dispensing_Collect Result.57', 'Dam dispensing_Collect Result.58', 'Dam dispensing_Collect Result.59', 'Dam dispensing_Collect Result.60', 'Dam dispensing_Collect Result.61', 'Dam dispensing_Collect Result.62', 'Dam dispensing_Collect Result.63', 'Dam dispensing_Collect Result.64', 'Dam dispensing_Collect Result.65', 'Dam dispensing_Collect Result.66', 'Dam dispensing_Collect Result.67', 'Dam dispensing_Collect Result.68', 'Dam dispensing_Collect Result.69', 'Auto clave_Model.Suffix', 'Auto clave_Workorder', 'Auto clave_LOT ID', 'Auto clave_Collect Date', 'Auto clave_Collect Result', 'Auto clave_Unit Time', 'Auto clave_Collect Result.1', 'Auto clave_Unit Time.1', 'Auto clave_Collect Result.2', 'Auto clave_Unit Time.2', 'Auto clave_Collect Result.3', 'Auto clave_Unit Time.3', 'Auto clave_Judge Value.3', 'Fill1 dispensing_Equipment', 'Fill1 dispensing_Model.Suffix', 'Fill1 dispensing_Workorder', 'Fill1 dispensing_LOT ID', 'Fill1 dispensing_Collect Date', 'Fill1 dispensing_Collect Result', 'Fill1 dispensing_Collect Result.1', 'Fill1 dispensing_Collect Result.2', 'Fill1 dispensing_Collect Result.3', 'Fill1 dispensing_Collect Result.4', 'Fill1 dispensing_Collect Result.5', 'Fill1 dispensing_Collect Result.6', 'Fill1 dispensing_Collect Result.7', 'Fill1 dispensing_Collect Result.8', 'Fill1 dispensing_Collect Result.9', 'Fill1 dispensing_Collect Result.10', 'Fill1 dispensing_Collect Result.11', 'Fill1 dispensing_Collect Result.12', 'Fill1 dispensing_Collect Result.13', 'Fill1 dispensing_Collect Result.14', 'Fill1 dispensing_Collect Result.15', 'Fill1 dispensing_Collect Result.16', 'Fill1 dispensing_Collect Result.17', 'Fill1 dispensing_Collect Result.18', 'Fill1 dispensing_Collect Result.19', 'Fill1 dispensing_Collect Result.20', 'Fill1 dispensing_Collect Result.21', 'Fill1 dispensing_Collect Result.22', 'Fill1 dispensing_Collect Result.23', 'Fill1 dispensing_Collect Result.24', 'Fill1 dispensing_Collect Result.25', 'Fill1 dispensing_Collect Result.26', 'Fill1 dispensing_Collect Result.27', 'Fill1 dispensing_Collect Result.28', 'Fill1 dispensing_Collect Result.29', 'Fill2 dispensing_Equipment', 'Fill2 dispensing_Model.Suffix', 'Fill2 dispensing_Workorder', 'Fill2 dispensing_LOT ID', 'Fill2 dispensing_Collect Date', 'Fill2 dispensing_Collect Result', 'Fill2 dispensing_Collect Result.1', 'Fill2 dispensing_Collect Result.3', 'Fill2 dispensing_Collect Result.5', 'Fill2 dispensing_Collect Result.7', 'Fill2 dispensing_Collect Result.8', 'Fill2 dispensing_Collect Result.17', 'Fill2 dispensing_Collect Result.18', 'Fill2 dispensing_Collect Result.19', 'Fill2 dispensing_Collect Result.20', 'Fill2 dispensing_Collect Result.21', 'Fill2 dispensing_Collect Result.22', 'Fill2 dispensing_Collect Result.23', 'Fill2 dispensing_Collect Result.24', 'Fill2 dispensing_Collect Result.25', 'Fill2 dispensing_Collect Result.26', 'Fill2 dispensing_Collect Result.27', 'Fill2 dispensing_Collect Result.28', 'Fill2 dispensing_Collect Result.29', 'Fill2 dispensing_Collect Result.30', 'Fill2 dispensing_Collect Result.31', 'Fill2 dispensing_Collect Result.32', 'Fill2 dispensing_Collect Result.33', 'Fill2 dispensing_Collect Result.34', 'Fill2 dispensing_Collect Result.35', 'Fill2 dispensing_Collect Result.36', 'Fill2 dispensing_Collect Result.37', 'Fill2 dispensing_Collect Result.38', 'Fill2 dispensing_Collect Result.39', 'target']\n",
      "Number of removed outliers: 2026\n",
      "Shape of cleaned dataframe: (38480, 153)\n",
      "\n",
      "Class distribution before outlier removal:\n",
      "target\n",
      "Normal      38156\n",
      "AbNormal     2350\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution after outlier removal:\n",
      "target\n",
      "Normal      36400\n",
      "AbNormal     2080\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of samples before outlier removal: 40506\n",
      "Total number of samples after outlier removal: 38480\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def remove_outliers_with_gmm(file_path, n_components, random_state):\n",
    "    \"\"\"\n",
    "    Remove outliers using Gaussian Mixture Models (GMM).\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the CSV file.\n",
    "    - n_components: int, the number of mixture components to use for GMM.\n",
    "    - random_state: int, random state for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - df_clean: DataFrame with the outliers removed.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Store original column order\n",
    "    original_columns = df.columns.tolist()\n",
    "\n",
    "    # Separate features and target\n",
    "    features = df.drop(columns=['target'])\n",
    "    target = df['target']\n",
    "\n",
    "    # Fit a GMM model to the data\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=random_state)\n",
    "    gmm.fit(features)\n",
    "\n",
    "    # Get the log-likelihood of each sample\n",
    "    log_likelihood = gmm.score_samples(features)\n",
    "\n",
    "    # Define a threshold to identify outliers\n",
    "    threshold = np.percentile(log_likelihood, 5)  # Removing the bottom 5% as outliers\n",
    "\n",
    "    # Identify outliers\n",
    "    outliers = log_likelihood < threshold\n",
    "\n",
    "    # Remove outliers from the dataset\n",
    "    df_clean = df[~outliers]\n",
    "\n",
    "    # Ensure the column order is maintained\n",
    "    df_clean = df_clean[original_columns]\n",
    "\n",
    "    return df_clean, df, outliers\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'PROCESSED.csv'  # Path to your CSV file\n",
    "n_components = 2  # Number of mixture components\n",
    "\n",
    "df_clean, df, outliers = remove_outliers_with_gmm(file_path, n_components, random_state)\n",
    "\n",
    "# Verify that column order is maintained\n",
    "print(\"Original columns:\", df.columns.tolist())\n",
    "print(\"Cleaned columns:\", df_clean.columns.tolist())\n",
    "\n",
    "#df_clean.to_csv('CLEANED_PROCESSED.csv', index=False)\n",
    "\n",
    "# Print the number of removed outliers and the shape of the cleaned dataframe\n",
    "print(f\"Number of removed outliers: {len(df) - len(df_clean)}\")\n",
    "print(f\"Shape of cleaned dataframe: {df_clean.shape}\")\n",
    "\n",
    "# Print class distribution before and after outlier removal\n",
    "print(\"\\nClass distribution before outlier removal:\")\n",
    "print(df['target'].value_counts())\n",
    "\n",
    "print(\"\\nClass distribution after outlier removal:\")\n",
    "print(df_clean['target'].value_counts())\n",
    "\n",
    "# Print the total number of samples before and after outlier removal\n",
    "print(f\"\\nTotal number of samples before outlier removal: {len(df)}\")\n",
    "print(f\"Total number of samples after outlier removal: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6118ae22",
   "metadata": {},
   "source": [
    "### 데이터 보간"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97966549",
   "metadata": {},
   "source": [
    "데이타 불균형을 해결하기 위해 언더/오버 샘플링을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be3d675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oss_resample(features, target, RANDOM_STATE, step_ratio=0.02):\n",
    "    \"\"\"\n",
    "    Perform One-Sided Selection (OSS) under-sampling.\n",
    "    \"\"\"\n",
    "    oss = OneSidedSelection(random_state=RANDOM_STATE)\n",
    "    X_oss, y_oss = oss.fit_resample(features, target)\n",
    "    \n",
    "    print('Performing OSS...')\n",
    "\n",
    "    # Calculate the number of samples needed to achieve the step ratio\n",
    "    initial_count_major = (y_oss == 'Normal').sum()\n",
    "    initial_count_minor = (y_oss == 'AbNormal').sum()\n",
    "\n",
    "    target_count_major = int(initial_count_major * (1 - step_ratio))\n",
    "    count_major = initial_count_major\n",
    "    print('Target ratio: ', target_count_major/initial_count_minor)\n",
    "\n",
    "    while count_major > target_count_major:\n",
    "        X_oss, y_oss = oss.fit_resample(X_oss, y_oss)\n",
    "        count_major = (y_oss == 'Normal').sum()\n",
    "        count_minor = (y_oss == 'AbNormal').sum()\n",
    "        print('count_major: ', count_major, 'count_minor: ', count_minor, 'Updated ratio:', count_major/count_minor)\n",
    "\n",
    "    return X_oss, y_oss\n",
    "\n",
    "def adasyn_resample(features, target, RANDOM_STATE, step_ratio=0.05):\n",
    "    \"\"\"\n",
    "    Perform ADASYN over-sampling.\n",
    "    \"\"\"\n",
    "    adasyn = ADASYN(random_state=RANDOM_STATE)\n",
    "\n",
    "    X_adasyn, y_adasyn = adasyn.fit_resample(features, target)\n",
    "    \n",
    "    print('Performing ADASYN...')\n",
    "    \n",
    "    # Calculate the number of samples needed to achieve the step ratio\n",
    "    initial_count_major = (y_adasyn == 'Normal').sum()\n",
    "    initial_count_minor = (y_adasyn == 'AbNormal').sum()\n",
    "\n",
    "    target_count_minor = int(initial_count_minor * (1 + step_ratio))\n",
    "    count_minor = initial_count_minor\n",
    "    print('initial count minor: ', initial_count_minor)\n",
    "    print('Target ratio: ', initial_count_major/target_count_minor)\n",
    "\n",
    "    '''while count_minor < target_count_minor:\n",
    "        X_adasyn, y_adasyn = adasyn.fit_resample(X_adasyn, y_adasyn)\n",
    "        count_major = (y_adasyn == 'Normal').sum()\n",
    "        count_minor = (y_adasyn == 'AbNormal').sum()\n",
    "        print('count_major: ', count_major, 'count_minor: ', count_minor, 'Updated ratio:', count_major/count_minor)'''\n",
    "\n",
    "    return X_adasyn, y_adasyn\n",
    "\n",
    "def sampling(df, RANDOM_STATE, target_ratio=1.5):\n",
    "    \"\"\"\n",
    "    Perform under-sampling and over-sampling to adjust the class distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data with 'target' column.\n",
    "    - RANDOM_STATE: int, random state for reproducibility.\n",
    "    - target_ratio: float, the desired ratio of Normal to AbNormal after sampling.\n",
    "    - step_ratio: float, the step ratio to apply in each under-sampling and over-sampling step.\n",
    "\n",
    "    Returns:\n",
    "    - df_concat: DataFrame with the sampled data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate the target column\n",
    "    target = df['target']\n",
    "    features = df.drop(columns=['target'])\n",
    "\n",
    "    # Initial class distribution\n",
    "    count_major = (target == 'Normal').sum()\n",
    "    count_minor = (target == 'AbNormal').sum()\n",
    "    current_ratio = count_major / count_minor\n",
    "    print('Initial class distribution - Normal:', count_major, 'AbNormal:', count_minor)\n",
    "    print('Initial ratio:', current_ratio)\n",
    "\n",
    "    while current_ratio > target_ratio:\n",
    "        # Perform One-Sided Selection (OSS) under-sampling\n",
    "        X_oss, y_oss = oss_resample(features, target, RANDOM_STATE, step_ratio=0.02)\n",
    "        df_oss = pd.concat([pd.DataFrame(X_oss), pd.Series(y_oss, name='target')], axis=1)\n",
    "\n",
    "        # Perform ADASYN over-sampling\n",
    "        X_adasyn, y_adasyn = adasyn_resample(df_oss.drop(columns=['target']), df_oss['target'], RANDOM_STATE, step_ratio=0.05)\n",
    "        df_adasyn = pd.concat([pd.DataFrame(X_adasyn), pd.Series(y_adasyn, name='target')], axis=1)\n",
    "\n",
    "        features = df_adasyn.drop(columns=['target'])\n",
    "        target = df_adasyn['target']\n",
    "\n",
    "        # Update class distribution\n",
    "        count_major = (target == 'Normal').sum()\n",
    "        count_minor = (target == 'AbNormal').sum()\n",
    "        current_ratio = count_major / count_minor\n",
    "        print('Updated class distribution - Normal:', count_major, 'AbNormal:', count_minor)\n",
    "        print('Updated ratio:', current_ratio)\n",
    "\n",
    "    df_concat = df_adasyn\n",
    "\n",
    "    # Reorder the columns to place 'target' at the front\n",
    "    columns = ['target'] + [col for col in df_concat.columns if col != 'target']\n",
    "    df_concat = df_concat[columns]\n",
    "\n",
    "    # Sort by date if needed\n",
    "    if 'Collect Date - Dam' in df_concat.columns:\n",
    "        df_concat = df_concat.sort_values(by=[\"Collect Date - Dam\"])\n",
    "\n",
    "    # Print the final counts\n",
    "    print(df_concat['target'].value_counts())\n",
    "\n",
    "    return df_concat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data merging...\n",
      "Initial class distribution - Normal: 36400 AbNormal: 2080\n",
      "Initial ratio: 17.5\n",
      "Performing OSS...\n",
      "Target ratio:  16.59326923076923\n",
      "count_major:  34722 count_minor:  2080 Updated ratio: 16.693269230769232\n",
      "count_major:  34536 count_minor:  2080 Updated ratio: 16.603846153846153\n",
      "count_major:  34390 count_minor:  2080 Updated ratio: 16.533653846153847\n",
      "Performing ADASYN...\n",
      "initial count minor:  34297\n",
      "Target ratio:  0.9549859765071783\n",
      "Updated class distribution - Normal: 34390 AbNormal: 34297\n",
      "Updated ratio: 1.0027116074292213\n",
      "target\n",
      "Normal      34390\n",
      "AbNormal    34297\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "print('Data merging...')\n",
    "sampled_df = sampling(df_clean, random_state, target_ratio=1.5)\n",
    "\n",
    "#processed_file_path = './SAMPLED_PROCESSED.csv'  # Path to save the sampled CSV file\n",
    "#sampled_df.to_csv(processed_file_path, index=False)\n",
    "#print(f\"Sampled data saved to {processed_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeaabc1",
   "metadata": {},
   "source": [
    "### 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"SAMPLED_PROCESSED.csv\", low_memory=False)\n",
    "df_train, df_val = train_test_split(\n",
    "    sampled_df,\n",
    "    test_size=0.1,\n",
    "    stratify=sampled_df[\"target\"],\n",
    "    random_state=random_state,\n",
    ")\n",
    "features = sampled_df.drop(columns=['target'])\n",
    "features = []\n",
    "\n",
    "for col in sampled_df.columns:\n",
    "    try:\n",
    "        df_train[col] = df_train[col].astype(int)\n",
    "        features.append(col)\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88851abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61818, 152) (6869, 152)\n"
     ]
    }
   ],
   "source": [
    "train_x = df_train[features]\n",
    "train_y = df_train[\"target\"]\n",
    "val_x = df_val[features]\n",
    "val_y = df_val[\"target\"]\n",
    "\n",
    "\n",
    "print(train_x.shape, val_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "149602a5-340c-45c9-8b6b-b327f27a957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost with Class Weighting\n",
    "weight_for_class_0 = sum(train_y == 'AbNormal') / len(train_y)\n",
    "weight_for_class_1 = sum(train_y == 'Normal') / len(train_y)\n",
    "model = CatBoostClassifier(class_weights=[weight_for_class_0, weight_for_class_1], random_state=random_state, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ecfa9b",
   "metadata": {},
   "source": [
    "## 3. 모델 학습 중 최고만 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88e37a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Set ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OP753345013050000002</td>\n",
       "      <td>AbNormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OP753345013050000005</td>\n",
       "      <td>AbNormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OP753345013050000006</td>\n",
       "      <td>AbNormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OP753345013050000008</td>\n",
       "      <td>AbNormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OP753345013050000009</td>\n",
       "      <td>AbNormal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Set ID    target\n",
       "0  OP753345013050000002  AbNormal\n",
       "1  OP753345013050000005  AbNormal\n",
       "2  OP753345013050000006  AbNormal\n",
       "3  OP753345013050000008  AbNormal\n",
       "4  OP753345013050000009  AbNormal"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_y = pd.read_csv(os.path.join(\"submission.csv\"))\n",
    "df_test_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "공통으로 존재하는 'Set ID' 값 개수: 17361\n",
      "공통으로 존재하는 'Set ID' 값 예시: ['OP753345013110001769', 'OP753345013100004575', 'OP753345083120000005', 'OP753345013050000332', 'OP753345014010000225', 'OP753345054040000518', 'OP753345013050003472', 'OP753345013120006858', 'OP753345013060003825', 'OP753345013100001433']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_139921/3332283810.py:2: DtypeWarning: Columns (62,64,244,246,278,280,409,411) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test = pd.read_csv(os.path.join(\"MERGED_TEST.csv\"))\n"
     ]
    }
   ],
   "source": [
    "# 공통으로 존재하는 'Set ID' 값 확인\n",
    "test = pd.read_csv(os.path.join(\"MERGED_TEST.csv\"))\n",
    "common_ids = set(test['Set ID']).intersection(set(df_test_y['Set ID']))\n",
    "print(\"공통으로 존재하는 'Set ID' 값 개수:\", len(common_ids))\n",
    "print(\"공통으로 존재하는 'Set ID' 값 예시:\", list(common_ids)[:10])\n",
    "\n",
    "df_test_x = test.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1eb2e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dam dispensing_Equipment  Dam dispensing_Model.Suffix  \\\n",
      "0                         0                            0   \n",
      "1                         0                            0   \n",
      "2                         0                            0   \n",
      "3                         0                            0   \n",
      "4                         0                            0   \n",
      "\n",
      "   Dam dispensing_Workorder  Dam dispensing_LOT ID  Set ID  \\\n",
      "0                         0                      0       0   \n",
      "1                         0                      1       1   \n",
      "2                         0                      2       2   \n",
      "3                         0                      3       3   \n",
      "4                         0                      4       4   \n",
      "\n",
      "   Dam dispensing_Collect Date  Dam dispensing_Collect Result  \\\n",
      "0                            0                      -0.785738   \n",
      "1                            1                      -0.785738   \n",
      "2                            2                      -0.785738   \n",
      "3                            3                      -0.785738   \n",
      "4                            4                      -0.785738   \n",
      "\n",
      "   Dam dispensing_Collect Result.1  Dam dispensing_Collect Result.2  \\\n",
      "0                        -0.785738                        -0.785738   \n",
      "1                        -0.785738                        -0.785738   \n",
      "2                        -0.785738                        -0.785738   \n",
      "3                        -0.785738                        -0.785738   \n",
      "4                        -0.785738                        -0.785738   \n",
      "\n",
      "   Dam dispensing_Collect Result.3  ...  Fill2 dispensing_Collect Result.31  \\\n",
      "0                        -0.403026  ...                            1.214866   \n",
      "1                        -0.403026  ...                            1.214866   \n",
      "2                        -0.403026  ...                            1.214866   \n",
      "3                        -0.403026  ...                            1.214866   \n",
      "4                        -0.403026  ...                            1.214866   \n",
      "\n",
      "   Fill2 dispensing_Collect Result.32  Fill2 dispensing_Collect Result.33  \\\n",
      "0                            1.214866                           -1.214866   \n",
      "1                            1.214866                           -1.214866   \n",
      "2                            1.214866                           -1.214866   \n",
      "3                            1.214866                           -1.214866   \n",
      "4                            1.214866                           -1.214866   \n",
      "\n",
      "   Fill2 dispensing_Collect Result.34  Fill2 dispensing_Collect Result.35  \\\n",
      "0                            1.214866                           -1.218198   \n",
      "1                            1.214866                           -1.196276   \n",
      "2                            1.214866                           -1.209977   \n",
      "3                            1.214866                           -1.196276   \n",
      "4                            1.214866                           -1.199016   \n",
      "\n",
      "   Fill2 dispensing_Collect Result.36  Fill2 dispensing_Collect Result.37  \\\n",
      "0                           -1.024976                           -0.688275   \n",
      "1                           -1.024976                           -0.659598   \n",
      "2                           -0.887204                           -0.650039   \n",
      "3                           -1.713833                           -0.630921   \n",
      "4                           -1.576062                           -0.621362   \n",
      "\n",
      "   Fill2 dispensing_Collect Result.38  Fill2 dispensing_Collect Result.39  \\\n",
      "0                           -0.825394                           -0.618845   \n",
      "1                           -0.825394                           -0.618845   \n",
      "2                           -0.825394                           -0.618845   \n",
      "3                           -0.825394                           -0.618845   \n",
      "4                           -0.825394                           -0.618845   \n",
      "\n",
      "   Fill2 dispensing_Unnamed: 131  \n",
      "0                            NaN  \n",
      "1                            NaN  \n",
      "2                            NaN  \n",
      "3                            NaN  \n",
      "4                            NaN  \n",
      "\n",
      "[5 rows x 162 columns]\n"
     ]
    }
   ],
   "source": [
    "# 가정: df_test_x는 이미 로드된 상태\n",
    "\n",
    "# Step 1: Remove columns where all rows have the same value\n",
    "df_test_x = df_test_x.loc[:, (df_test_x != df_test_x.iloc[0]).any()]\n",
    "\n",
    "# Step 2: Remove columns where all rows have missing values\n",
    "df_test_x = df_test_x.dropna(axis=1, how='all')\n",
    "\n",
    "# Step 3: Categorize non-numerical columns\n",
    "non_numeric_features = df_test_x.select_dtypes(exclude=[float, int])\n",
    "for column in non_numeric_features.columns:\n",
    "    df_test_x[column] = pd.Categorical(df_test_x[column]).codes\n",
    "\n",
    "# Step 4: Scale numerical columns\n",
    "numeric_features = df_test_x.select_dtypes(include=[float, int])\n",
    "scaler = StandardScaler()\n",
    "scaled_numeric_features = scaler.fit_transform(numeric_features)\n",
    "\n",
    "# Replace original numeric columns with scaled columns\n",
    "df_test_x[numeric_features.columns] = scaled_numeric_features\n",
    "\n",
    "# 최종 처리된 DataFrame 출력\n",
    "print(df_test_x.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacd5ed8",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2904ef0e-c3bb-4bcc-8d5d-ef2a98dcde32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled Validation F1 Score: 0.5349777598059038\n"
     ]
    }
   ],
   "source": [
    "# 언더샘플링 및 오버샘플링을 위한 파이프라인 구성\n",
    "'''over = SMOTE(random_state=RANDOM_STATE)\n",
    "under = RandomUnderSampler(random_state=RANDOM_STATE)\n",
    "pipeline = Pipeline(steps=[('o', over), ('u', under)])\n",
    "\n",
    "# 오버샘플링 및 언더샘플링 적용\n",
    "train_x_resampled, train_y_resampled = pipeline.fit_resample(train_x, train_y)'''\n",
    "\n",
    "# 클래스 가중치 계산\n",
    "weight_for_class_0 = sum(train_y == 'AbNormal') / len(train_y)\n",
    "weight_for_class_1 = sum(train_y == 'Normal') / len(train_y)\n",
    "\n",
    "\n",
    "# 재학습\n",
    "#model.fit(train_x_resampled, train_y_resampled)\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "# 재평가\n",
    "train_pred = model.predict(train_x)\n",
    "val_pred = model.predict(val_x)\n",
    "\n",
    "# print(RANDOM_STATE)\n",
    "now_best = f1_score(val_y, val_pred, pos_label='AbNormal')\n",
    "print(\"Resampled Validation F1 Score:\", now_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 데이터 불러오기 및 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/9\n",
      "(61055, 152) (7632, 152)\n",
      "Resampled Validation F1 Score: 0.9701472376063758\n",
      "Best model updated with F1 Score: 0.9701472376063758\n",
      "Fold 2/9\n",
      "(61055, 152) (7632, 152)\n",
      "Resampled Validation F1 Score: 0.9687542269714595\n",
      "Fold 3/9\n",
      "(61055, 152) (7632, 152)\n",
      "Resampled Validation F1 Score: 0.971266693646297\n",
      "Best model updated with F1 Score: 0.971266693646297\n",
      "Fold 4/9\n",
      "(61055, 152) (7632, 152)\n",
      "Resampled Validation F1 Score: 0.9697379086733315\n",
      "Fold 5/9\n",
      "(61055, 152) (7632, 152)\n",
      "Resampled Validation F1 Score: 0.9670865501828525\n",
      "Fold 6/9\n",
      "(61055, 152) (7632, 152)\n",
      "Resampled Validation F1 Score: 0.9644021739130435\n",
      "Fold 7/9\n",
      "(61055, 152) (7632, 152)\n",
      "Resampled Validation F1 Score: 0.9720836142953473\n",
      "Best model updated with F1 Score: 0.9720836142953473\n",
      "Fold 8/9\n",
      "(61055, 152) (7632, 152)\n",
      "Resampled Validation F1 Score: 0.9700162074554295\n",
      "Fold 9/9\n",
      "(61056, 152) (7631, 152)\n",
      "Resampled Validation F1 Score: 0.9668066657634466\n",
      "\n",
      "Best F1 Score achieved: 0.9720836142953473\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# RANDOM_STATE를 설정합니다.\n",
    "RANDOM_STATE = 881\n",
    "\n",
    "# 데이터셋 및 모델 설정\n",
    "# df_concat: 전체 데이터프레임\n",
    "# features: 사용하려는 특성(컬럼) 목록\n",
    "# model: 사용할 모델 객체 (예: RandomForestClassifier, XGBoostClassifier 등)\n",
    "X = sampled_df[features]\n",
    "y = sampled_df[\"target\"]\n",
    "\n",
    "# StratifiedKFold 객체 생성\n",
    "skf = StratifiedKFold(n_splits=9, shuffle=True, random_state=RANDOM_STATE)\n",
    "best = 0\n",
    "best_model = None\n",
    "\n",
    "# 교차 검증을 위한 데이터셋 분할 및 반복\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold {fold + 1}/{skf.n_splits}\")\n",
    "    \n",
    "    train_x, val_x = X.iloc[train_index], X.iloc[val_index]\n",
    "    train_y, val_y = y.iloc[train_index], y.iloc[val_index]\n",
    "    print(train_x.shape, val_x.shape)\n",
    "    \n",
    "    # 언더샘플링 및 오버샘플링을 위한 파이프라인 구성\n",
    "    over = SMOTE(random_state=RANDOM_STATE)\n",
    "    under = RandomUnderSampler(random_state=RANDOM_STATE)\n",
    "    pipeline = Pipeline(steps=[('o', over), ('u', under)])\n",
    "    \n",
    "    # 오버샘플링 및 언더샘플링 적용\n",
    "    train_x_resampled, train_y_resampled = pipeline.fit_resample(train_x, train_y)\n",
    "    \n",
    "    # 클래스 가중치 계산\n",
    "    weight_for_class_0 = sum(train_y == 'AbNormal') / len(train_y)\n",
    "    weight_for_class_1 = sum(train_y == 'Normal') / len(train_y)\n",
    "    \n",
    "    # 모델 재학습\n",
    "    model.fit(train_x_resampled, train_y_resampled)\n",
    "    \n",
    "    # 재평가\n",
    "    train_pred_resampled = model.predict(train_x_resampled)\n",
    "    val_pred_resampled = model.predict(val_x)\n",
    "    \n",
    "    # F1 Score 계산\n",
    "    now_best = f1_score(val_y, val_pred_resampled, pos_label='AbNormal')\n",
    "    print(\"Resampled Validation F1 Score:\", now_best)\n",
    "    \n",
    "    # 최고의 모델 업데이트\n",
    "    if now_best > best:\n",
    "        best = now_best\n",
    "        best_model = model\n",
    "        print(\"Best model updated with F1 Score:\", best)\n",
    "\n",
    "# 최고의 모델을 저장하거나 필요한 후처리를 진행합니다.\n",
    "# 예: 모델을 파일에 저장\n",
    "\n",
    "joblib.dump(best_model, 'best_model.pkl')\n",
    "\n",
    "print(f\"\\nBest F1 Score achieved: {best}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation F1 Score with best model: 0.9668066657634466\n"
     ]
    }
   ],
   "source": [
    "# 저장된 모델을 불러와서 val_x와 val_y로 검증 F1 스코어를 측정합니다.\n",
    "loaded_model = joblib.load('best_model.pkl')\n",
    "val_pred_final = loaded_model.predict(val_x)\n",
    "final_f1_score = f1_score(val_y, val_pred_final, pos_label='AbNormal')\n",
    "print(f\"Final Validation F1 Score with best model: {final_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42697fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9668066657634466 is best\n"
     ]
    }
   ],
   "source": [
    "test_pred = loaded_model.predict(df_test_x)\n",
    "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub[\"target\"] = test_pred\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"submission.csv\", index=False)\n",
    "print(now_best ,\"is best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf8300",
   "metadata": {},
   "source": [
    "## 4. 제출하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f18e6a",
   "metadata": {},
   "source": [
    "### 제출 파일 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # StratifiedKFold 객체 생성\n",
    "# skf = KFold(n_splits=9, shuffle=True, random_state=RANDOM_STATE)\n",
    "# best = 0\n",
    "# # features와 target 설정\n",
    "# X = df_concat[features]\n",
    "# y = df_concat[\"target\"]\n",
    "\n",
    "# # 교차 검증을 위한 데이터셋 분할 및 반복\n",
    "# for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "#     train_x, val_x = X.iloc[train_index], X.iloc[val_index]\n",
    "#     train_y, val_y = y.iloc[train_index], y.iloc[val_index]\n",
    "#     print(train_x.shape, val_x.shape)\n",
    "#     RANDOM_STATE = 881\n",
    "#     # 언더샘플링 및 오버샘플링을 위한 파이프라인 구성\n",
    "#     over = SMOTE(random_state=RANDOM_STATE)\n",
    "#     under = RandomUnderSampler(random_state=RANDOM_STATE)\n",
    "#     pipeline = Pipeline(steps=[('o', over), ('u', under)])\n",
    "    \n",
    "#     # 오버샘플링 및 언더샘플링 적용\n",
    "#     train_x_resampled, train_y_resampled = pipeline.fit_resample(train_x, train_y)\n",
    "    \n",
    "#     # 클래스 가중치 계산\n",
    "#     weight_for_class_0 = sum(train_y == 'AbNormal') / len(train_y)\n",
    "#     weight_for_class_1 = sum(train_y == 'Normal') / len(train_y)\n",
    "    \n",
    "    \n",
    "#     # 재학습\n",
    "#     model.fit(train_x_resampled, train_y_resampled)\n",
    "    \n",
    "#     # 재평가\n",
    "#     train_pred_resampled = model.predict(train_x_resampled)\n",
    "#     val_pred_resampled = model.predict(val_x)\n",
    "    \n",
    "#     # print(RANDOM_STATE)\n",
    "#     now_best = f1_score(val_y, val_pred_resampled, pos_label='AbNormal')\n",
    "#     print(\"Resampled Validation F1 Score:\", now_best)\n",
    "#     if now_best>best:\n",
    "#         best = now_best\n",
    "#         best_model = model\n",
    "#         print(\"best updated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7867ce",
   "metadata": {},
   "source": [
    "**우측 상단의 제출 버튼을 클릭해 결과를 확인하세요**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
