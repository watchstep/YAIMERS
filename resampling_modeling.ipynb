{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ROOT_DIR = \"./data\"\n",
    "RANDOM_SEED = 110\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "df_tr = pd.read_csv(os.path.join(ROOT_DIR, \"train_v1.csv\"))\n",
    "df_te = pd.read_csv(os.path.join(ROOT_DIR, \"test_v1.csv\"))\n",
    "df_list = [df_tr, df_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    df.drop([\"Workorder\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling & Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\"Equipment_Dam\",\n",
    "                \"Equipment_Fill1\",\n",
    "                \"Equipment_Fill2\",\n",
    "                \"Model.Suffix\",\n",
    "                # \"Workorder\",\n",
    "                \"Workorder Category\",\n",
    "                \"Chamber Temp. Judge Value_AutoClave\"]\n",
    "\n",
    "# 여기서 Workorder 빼기로 함\n",
    "\n",
    "bins_features = df_tr.columns[df_tr.columns.str.contains(r\".*Bins.*\")].tolist()\n",
    "from_bins_features = [re.sub(r'\\s*Bins\\s*', '', f).strip() for f in bins_features]\n",
    "\n",
    "cat_features.extend(bins_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    df[cat_features] = df[cat_features].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = df_tr.select_dtypes(exclude=[\"category\"]).columns.to_list()\n",
    "num_features.remove(\"target\")\n",
    "\n",
    "all_features = num_features + cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우리가 파악해야하는 건 AbNormal이므로 1로 설정\n",
    "df_tr[\"target\"] = df_tr[\"target\"].map({\"Normal\": 0, \"AbNormal\": 1})\n",
    "# df_tr[\"target\"] = df_tr[\"target\"].map({0:\"Normal\", 1:\"AbNormal\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = df_tr.drop(\"target\", axis=1)\n",
    "y_tr = df_tr[\"target\"]\n",
    "\n",
    "X_te = df_te.drop(\"Set ID\", axis=1)\n",
    "set_id = df_te[\"Set ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct = make_column_transformer(\n",
    "#     (OneHotEncoder(handle_unknown=\"ignore\", sparse=False), cat_features),\n",
    "#     (MinMaxScaler(), num_features),\n",
    "#     remainder=\"passthrough\",\n",
    "# )\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for cat_feature in cat_features:\n",
    "    X_tr[cat_feature] = le.fit_transform(X_tr[cat_feature])\n",
    "    X_te[cat_feature] = le.transform(X_te[cat_feature])\n",
    "   \n",
    "\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "X_tr[num_features] = mms.fit_transform(X_tr[num_features])\n",
    "X_te[num_features] = mms.transform(X_te[num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr[\"target\"].value_counts().plot(kind=\"barh\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_pie(df):\n",
    "    df[\"target\"].value_counts().plot.pie(labels=[\"Normal\", \"AbNormal\"], autopct=lambda x:f\"{x:.2f}%\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_pie(df_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe = ct.transformers_[0][1]\n",
    "# ohe_cat_features = ohe.get_feature_names_out(cat_features)\n",
    "# ohe_all_features = num_features + list(ohe_cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from imblearn.under_sampling import (NearMiss,\n",
    "                                     ClusterCentroids,\n",
    "                                     AllKNN,\n",
    "                                     OneSidedSelection,\n",
    "                                     TomekLinks)\n",
    "from imblearn.over_sampling import (RandomOverSampler,\n",
    "                                    SMOTE,\n",
    "                                    ADASYN,\n",
    "                                    SMOTENC,\n",
    "                                    BorderlineSMOTE,\n",
    "                                    KMeansSMOTE)\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "def random_downsample(df, sample_ratio=1.0, random_seed=RANDOM_SEED):\n",
    "    df_normal = df[df[\"target\"] == 0] \n",
    "    df_abnormal = df[df[\"target\"] == 1]\n",
    "    \n",
    "    downsampled = resample(\n",
    "        df_normal,\n",
    "        replace=False,\n",
    "        n_samples=int(len(df_abnormal) * sample_ratio),\n",
    "        random_state=random_seed\n",
    "    )\n",
    "    \n",
    "    downsampled_df = pd.concat([df_abnormal, downsampled])\n",
    "    \n",
    "    return downsampled_df\n",
    "\n",
    "def downsample(X, y, method, random_seed=RANDOM_SEED):\n",
    "    # NearMiss\n",
    "    if method == \"nearmiss\":\n",
    "        # sampling_strategy=\"auto\"\n",
    "        nm = NearMiss(sampling_strategy=0.4)\n",
    "        X_downsampled, y_downsampled = nm.fit_resample(X, y)\n",
    "    # ClusterCentroids\n",
    "    elif method == \"cluster\":\n",
    "        cc = ClusterCentroids(random_state=random_seed)\n",
    "        X_downsampled, y_downsampled = cc.fit_resample(X, y)\n",
    "    # AllKNN\n",
    "    elif method == \"allknn\":\n",
    "        allknn = AllKNN()\n",
    "        X_downsampled, y_downsampled = allknn.fit_resample(X, y)\n",
    "    # OneSidedSelection\n",
    "    elif method == \"oneside\":\n",
    "        oss = OneSidedSelection(random_state=random_seed)\n",
    "        X_downsampled, y_downsampled = oss.fit_resample(X, y)\n",
    "    # Tomeklinks\n",
    "    elif method == \"tomek\":\n",
    "        tl = TomekLinks()\n",
    "        X_downsampled, y_downsampled = tl.fit_resample(X, y)\n",
    "    \n",
    "    X_downsampled_df= pd.DataFrame(X_downsampled, columns=all_features)\n",
    "    y_downsampled_df = pd.Series(y_downsampled, name=\"target\") \n",
    "    downsampled_df = pd.concat([X_downsampled_df, y_downsampled_df], axis=1)\n",
    "    \n",
    "    print('Original dataset shape %s' % Counter(y))\n",
    "    print('Resampled dataset shape %s' % Counter(y_downsampled))\n",
    "    \n",
    "    return downsampled_df \n",
    "\n",
    "def upsample(X, y, cat_idx, method, random_seed=RANDOM_SEED):\n",
    "    # X = df.drop(\"target\", axis=1)\n",
    "    # y = df[\"target\"]\n",
    "    \n",
    "    if method == \"random\":\n",
    "        ros = RandomOverSampler(random_state=random_seed)\n",
    "        X_upsampled, y_upsampled = ros.fit_resample(X, y)\n",
    "    # SMOTE\n",
    "    if method == \"smote\":\n",
    "        smote = SMOTE(random_state=random_seed)\n",
    "        X_upsampled, y_upsampled = smote.fit_resample(X, y)\n",
    "    # ADASYN\n",
    "    elif method == \"adasyn\":\n",
    "        adasyn = ADASYN(random_state=random_seed)\n",
    "        X_upsampled, y_upsampled = adasyn.fit_resample(X, y)\n",
    "    # SMOTE-NC (both numerical & categorical features)\n",
    "    elif method == \"smotenc\":\n",
    "        smotenc = SMOTENC(random_state=random_seed, sampling_strategy=\"auto\", categorical_features=cat_idx)\n",
    "        X_upsampled, y_upsampled = smotenc.fit_resample(X, y)\n",
    "    elif method == \"borderline\":\n",
    "        borderline_smote = BorderlineSMOTE(random_state=random_seed)\n",
    "        X_upsampled, y_upsampled = borderline_smote.fit_resample(X, y)\n",
    "    elif method == \"kmeans\":\n",
    "        kmeans_smote = KMeansSMOTE(random_state=random_seed)\n",
    "        X_upsampled, y_upsampled = kmeans_smote.fit_resample(X, y)\n",
    "        \n",
    "    X_upsampled_df= pd.DataFrame(X_upsampled, columns=X_tr.columns)\n",
    "    y_upsampled_df = pd.Series(y_upsampled, name=\"target\") \n",
    "    upsampled_df = pd.concat([X_upsampled_df, y_upsampled_df], axis=1)\n",
    "    \n",
    "    print('Original dataset shape %s' % Counter(y))\n",
    "    print('Resampled dataset shape %s' % Counter(y_upsampled))\n",
    "    \n",
    "    return upsampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampled_df = random_downsample(df, sample_ratio=3.0)\n",
    "downsample_options = {1:\"nearmiss\", 2:\"cluster\", 3:\"allknn\", 4:\"oneside\", 5:\"tomek\"}\n",
    "\n",
    "downsampled_df_tr = downsample(X_tr, y_tr, method=downsample_options[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_df_tr.columns[downsampled_df_tr.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_pie(downsampled_df_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_options = {1: \"random\", 2:\"smote\", 3:\"adasyn\", 4:\"smotenc\", 5:\"borderline\", 6:\"kmeans\"}\n",
    "\n",
    "cat_idx = [downsampled_df_tr.columns.get_loc(col) for col in cat_features]\n",
    "X_tr = downsampled_df_tr.drop(\"target\", axis=1)\n",
    "y_tr = downsampled_df_tr[\"target\"]\n",
    "\n",
    "upsampled_df_tr = upsample(X_tr, y_tr, cat_idx, method=upsample_options[4])\n",
    "# Counter(upsampled_df_tr['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_df_tr.columns[downsampled_df_tr.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_pie(upsampled_df_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = upsampled_df_tr.drop(\"target\", axis=1)\n",
    "y_tr = upsampled_df_tr[\"target\"]\n",
    "\n",
    "# X_tr = downsampled_df_tr.drop(\"target\", axis=1)\n",
    "# y_tr = downsampled_df_tr[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection / Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE, SelectFromModel, SelectKBest, f_classif, chi2\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_auc_score, precision_score, recall_score\n",
    "\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"cat\": CatBoostClassifier(random_state=RANDOM_SEED, auto_class_weights=\"Balanced\"),\n",
    "    \"lgbm\": LGBMClassifier(random_state=RANDOM_SEED,),\n",
    "    \"xgb\": XGBClassifier(random_state=RANDOM_SEED, eval_metric='auc', objective=\"binary:logistic\"),\n",
    "    \"ada\": AdaBoostClassifier(random_state=RANDOM_SEED),\n",
    "    \"rfc\": RandomForestClassifier(random_state=RANDOM_SEED, class_weight='balanced'),\n",
    "    \"lr\": LogisticRegression(random_state=RANDOM_SEED),\n",
    "    \"extra\": ExtraTreesClassifier(random_state=RANDOM_SEED)\n",
    "}\n",
    "\n",
    "log_cols = [\"model\", \"resample\", \"f1\", \"aucroc\", \"precision\", \"recall\", \"accuracy\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "DOWN = \"AllKNN\"\n",
    "UP = \"SMOTENC\"\n",
    "RESAMPLE = f\"{DOWN}-{UP}\"\n",
    "\n",
    "def get_log(clf, X, y):\n",
    "    y_prob = clf.predict_proba(X)\n",
    "    y_pred = clf.predict(X)\n",
    "    \n",
    "    clf_name = clf.__class__.__name__\n",
    "    f1 = f1_score(y,y_pred)\n",
    "    aucroc = roc_auc_score(y, y_prob[:,1])\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    \n",
    "    tmp_log = [clf_name, RESAMPLE, f1, aucroc, precision, recall, acc]\n",
    "    # log = pd.concat([log, tmp_log], ignore_index=True)\n",
    "    \n",
    "    # print(classification_report(y, y_pred))\n",
    "    \n",
    "    return tmp_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Anomaly Detection')\n",
    "\n",
    "parser.add_argument('--data_path', type=str, default='./data')\n",
    "parser.add_argument('--seed',type=int, default=110)\n",
    "\n",
    "parser.add_argument('--model', type=str, default='cat')\n",
    "\n",
    "parser.add_argument('-en', '--encoding', type=str, default='le')\n",
    "parser.add_argument('-s', '--scaling', type=str, default='mms')\n",
    "\n",
    "downsample_options = {1:\"nearmiss\", 2:\"cluster\", 3:\"allknn\", 4:\"oneside\", 5:\"tomek\"}\n",
    "parser.add_argument('-ds', '--downsampling', type=int, default=5)\n",
    "\n",
    "upsample_options = {1: \"random\", 2:\"smote\", 3:\"adasyn\", 4:\"smotenc\", 5:\"borderline\", 6:\"kmeans\"}\n",
    "parser.add_argument('-us', '--upsampling', type=int, default=4)\n",
    "\n",
    "parser.add_argument('-FS', type=bool, default=False, help='feature selection T:auto F:manual')\n",
    "parser.add_argument('--selector', type=str, default='sfm', help='auto feature selector')\n",
    "\n",
    "parser.add_argument('--check_all', type=bool, default=False)\n",
    "parser.add_argument('--is_tune', type=bool, default=False, help='optuna tuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classifiers[\"extra\"]\n",
    "model.fit(X_tr, y_tr)\n",
    "\n",
    "rfe = RFE(estimator=model, n_features_to_select=50)                                 \n",
    "sfm = SelectFromModel(estimator=model, threshold=\"mean\")\n",
    "kbest = SelectKBest(score_func=f_classif,)\n",
    "\n",
    "X_tr_selec = sfm.fit_transform(X_tr, y_tr)\n",
    "X_te_selec = sfm.transform(X_te)\n",
    "\n",
    "# # 수동\n",
    "# selected_features = [feature for feature in all_features if feature not in from_bins_features] # 기존 열 대신 Bins 열 사용\n",
    "# X_tr_selec = X_tr[selected_features]\n",
    "# X_te_selec = X_te[selected_features]\n",
    "\n",
    "print(\"Before \", X_tr.shape[1])\n",
    "print(\"After \", X_tr_selec.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before \", X_tr.shape[1])\n",
    "print(\"After \", X_tr_selec.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stk = StratifiedKFold(n_splits=5, random_state=RANDOM_SEED, shuffle=True)\n",
    "rstk = RepeatedStratifiedKFold(n_splits=5, random_state=RANDOM_SEED)\n",
    "\n",
    "# classifiers_lst = list(classifiers.values())\n",
    "\n",
    "# # 기본 확인\n",
    "# score_dic = {}\n",
    "# for clf_name, clf in classifiers.items():\n",
    "#     scores = cross_val_score(clf, X_tr_selec, y_tr, scoring=\"f1\", cv=stk)\n",
    "#     score_dic[clf_name] = scores.mean()\n",
    "\n",
    "scores = cross_val_score(classifiers[\"cat\"], X_tr_selec, y_tr, scoring=\"f1\", cv=stk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores.mean()\n",
    "score_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.0001, 0.2),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_uniform('colsample_bylevel', 0.5, 1.0),\n",
    "        'od_type': trial.suggest_categorical(\"od_type\", [\"IncToDec\", \"Iter\"]),\n",
    "        'od_wait': trial.suggest_int(\"od_wait\", 10, 50),\n",
    "    }\n",
    "\n",
    "    cat_clf = CatBoostClassifier(**params, random_state=RANDOM_SEED, auto_class_weights=\"Balanced\",) # eval_metric=\"TotalF1\"\n",
    "    \n",
    "    stk = StratifiedKFold(n_splits=5, random_state=RANDOM_SEED, shuffle=True)\n",
    "    f1_scores = np.empty(5)\n",
    "    \n",
    "    for idx, (tr_idx, val_idx) in enumerate(stk.split(X_tr_selec, y_tr)):\n",
    "        X_tr_fold, X_val_fold = X_tr.iloc[tr_idx], X_tr.iloc[val_idx]\n",
    "        y_tr_fold, y_val_fold = y_tr.iloc[tr_idx], y_tr.iloc[val_idx]\n",
    "        \n",
    "        cat_clf.fit(X_tr_fold, y_tr_fold, eval_set=[(X_val_fold, y_val_fold)], early_stopping_rounds=50, verbose=False)\n",
    "        y_pred_fold = cat_clf.predict(X_val_fold)\n",
    "        f1_scores[idx] = f1_score(y_val_fold, y_pred_fold)\n",
    "\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "cat_study = optuna.create_study(direction='maximize')\n",
    "cat_study.optimize(catboost_objective, n_trials=15)\n",
    "\n",
    "cat_best_params = cat_study.best_params\n",
    "cat_best_score = cat_study.best_value\n",
    "print(\"CatBoost Best Hyperparams: \", cat_best_params)\n",
    "print(\"CatBoost Best F1 Score: \", cat_best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_objective(trial, X_tr, y_tr):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 1000),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
    "    }\n",
    "    \n",
    "    lgbm_clf = LGBMClassifier(**params, random_state=RANDOM_SEED)\n",
    "    \n",
    "    stk = StratifiedKFold(n_splits=5, random_state=RANDOM_SEED, shuffle=True)\n",
    "    f1_scores = np.empty(5)\n",
    "    \n",
    "    for idx, (tr_idx, val_idx) in enumerate(stk.split(X_tr, y_tr)):\n",
    "        X_tr_fold, X_val_fold = X_tr.iloc[tr_idx], X_tr.iloc[val_idx]\n",
    "        y_tr_fold, y_val_fold = y_tr.iloc[tr_idx], y_tr.iloc[val_idx]\n",
    "        \n",
    "        lgbm_clf.fit(X_tr_fold, y_tr_fold, early_stopping_rounds=50, verbose=False)\n",
    "        y_pred_fold = lgbm_clf.predict(X_val_fold)\n",
    "        f1_scores[idx] = f1_score(y_val_fold, y_pred_fold)\n",
    "\n",
    "    return np.mean(f1_scores)\n",
    "    \n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(lgbm_objective, n_trials=15)\n",
    "\n",
    "# lgbm_best_params = study.best_params\n",
    "# lgbm_best_score= study.best_value\n",
    "# print(\"LGBM Best Hyperparams: \",lgbm_best_params)\n",
    "# print(\"LGBM Best F1 Score: \", lgbm_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clf = CatBoostClassifier(**tmp, random_state=RANDOM_SEED, auto_class_weights=\"Balanced\",)\n",
    "# final_clf = LGBMClassifier(**lgbm_best_params, random_state=RANDOM_SEED)\n",
    "\n",
    "final_clf.fit(X_tr_selec, y_tr, use_best_model=True)\n",
    "final_preds = final_clf.predict(X_te_selec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(os.path.join(ROOT_DIR, \"submission.csv\"))\n",
    "df_sub[\"target\"] = final_preds\n",
    "df_sub[\"target\"] = df_sub[\"target\"].map({0:\"Normal\", 1:\"AbNormal\"})\n",
    "df_sub[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "curr_date = datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "df_sub.to_csv(os.path.join(ROOT_DIR, f\"submission_{curr_date}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
