{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# IMPORT LIBRARY  #################################\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import pickle\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# https://contrib.scikit-learn.org/category_encoders/index.html\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.feature_selection import RFE, SelectFromModel, SelectKBest, f_classif, chi2\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_auc_score, precision_score, recall_score\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pycaret\n",
    "from pycaret.classification import *\n",
    "\n",
    "pd.options.display.max_columns = 200\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################   CONFIG  #######################\n",
    "parser = argparse.ArgumentParser(description='Anomaly Detection')\n",
    "\n",
    "parser.add_argument('--data_path', type=str, default='./data')\n",
    "parser.add_argument('--seed',type=int, default=110)\n",
    "\n",
    "parser.add_argument('--model', type=str, default='cat')\n",
    "\n",
    "parser.add_argument('-en', '--encoder', type=str, default='ohe')\n",
    "parser.add_argument('-s', '--scaler', type=str, default='mms')\n",
    "\n",
    "downsample_options = {1:\"nearmiss\", 2:\"cluster\", 3:\"allknn\", 4:\"oneside\", 5:\"tomek\"}\n",
    "parser.add_argument('-ds', '--downsampling', type=int, default=5) # TOMEK\n",
    "\n",
    "upsample_options = {1: \"random\", 2:\"smote\", 3:\"adasyn\", 4:\"smotenc\", 5:\"smoten\", 6:\"borderline\", 7:\"kmeans\", 8:\"svm\"}\n",
    "parser.add_argument('-us', '--upsampling', type=int, default=3) # SMOTEE - NC\n",
    "\n",
    "parser.add_argument('--fs_mode', type=bool, default=False, help='feature selection T:auto F:manual')\n",
    "parser.add_argument('--estimator', type=str, default='extra', help=\"using for feature selection\")\n",
    "parser.add_argument('--selector', type=str, default='sfm', help='auto feature selector')\n",
    "\n",
    "parser.add_argument('--check_all', type=bool, default=False)\n",
    "parser.add_argument('--tune_mode', type=bool, default=True, help='optuna tuning')\n",
    "\n",
    "config = parser.parse_args([])\n",
    "\n",
    "exp_config = f\"{config.encoder}_{config.scaler}_{downsample_options[config.downsampling]}_{upsample_options[config.upsampling]}\"\n",
    "\n",
    "random.seed(config.seed)\n",
    "np.random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################   LOAD DATA  #######################\n",
    "df_tr = pd.read_csv(os.path.join(config.data_path, \"train_v2.csv\"))\n",
    "df_te = pd.read_csv(os.path.join(config.data_path, \"test_v2.csv\"))\n",
    "df_list = [df_tr, df_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################  FEATURE HANDLING  ###########################\n",
    "## CATEGORICAL FEATURES\n",
    "cat_features = [\"Equipment_Dam\",\n",
    "                \"Equipment_Fill1\",\n",
    "                \"Equipment_Fill2\",\n",
    "                \"Model.Suffix\",\n",
    "                \"Workorder Category\",\n",
    "                \"Chamber Temp. Judge Value_AutoClave\"]\n",
    "\n",
    "## BINNING FEATURES\n",
    "bins_features = df_tr.columns[df_tr.columns.str.contains(r\".*Bins.*\")].tolist()\n",
    "# Bins 열 만드는 데 사용된 열\n",
    "from_bins_features = [re.sub(r'\\s*Bins\\s*', '', f).strip() for f in bins_features]\n",
    "\n",
    "cat_features.extend(bins_features)\n",
    "\n",
    "for df in df_list:\n",
    "    df[cat_features] = df[cat_features].astype(\"category\")\n",
    "\n",
    "## NUMERICAL FEATURES\n",
    "num_features = df_tr.select_dtypes(exclude=[\"category\"]).columns.to_list()\n",
    "num_features.remove(\"target\")\n",
    "\n",
    "## ALL FEATURES\n",
    "all_features = num_features + cat_features\n",
    "\n",
    "## TARGET ENCODING\n",
    "df_tr[\"target\"] = df_tr[\"target\"].map({\"Normal\": 0, \"AbNormal\": 1})\n",
    "    \n",
    "## DATA SPLITTING \n",
    "X_tr, y_tr = df_tr.drop(\"target\", axis=1), df_tr[\"target\"]\n",
    "X_te = df_te.drop(\"Set ID\", axis=1)\n",
    "\n",
    "##\n",
    "# for df in [X_tr, X_te]:\n",
    "#     df.columns = df.columns.str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################  FEATURE ENCODING/SCALING ###########################\n",
    "## ENCODING\n",
    "if config.encoder == \"le\":\n",
    "    le = LabelEncoder()\n",
    "    for cat_feature in cat_features:\n",
    "        X_tr[cat_feature] = le.fit_transform(X_tr[cat_feature])\n",
    "        X_te[cat_feature] = le.transform(X_te[cat_feature])\n",
    "        \n",
    "elif config.encoder == \"js\":\n",
    "    js = ce.JamesSteinEncoder(cols=cat_features)\n",
    "    \n",
    "    X_tr = js.fit_transform(X_tr, y_tr)\n",
    "    X_te = js.transform(X_te)\n",
    "    \n",
    "elif config.encoder == \"woe\":\n",
    "    woe = ce.WOEEncoder(cols=cat_features)\n",
    "    \n",
    "    X_tr = woe.fit_transform(X_tr, y_tr)\n",
    "    X_te = woe.transform(X_te)\n",
    "    \n",
    "elif config.encoder == \"ohe\": \n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    \n",
    "    encoded_tr = ohe.fit_transform(X_tr[cat_features])\n",
    "    encoded_df_tr = pd.DataFrame(encoded_tr, columns=ohe.get_feature_names_out())\n",
    "    X_tr = pd.concat([X_tr[num_features], encoded_df_tr], axis=1)\n",
    "    \n",
    "    encoded_te = ohe.transform(X_te[cat_features])\n",
    "    encoded_df_te = pd.DataFrame(encoded_te, columns=ohe.get_feature_names_out())\n",
    "    X_te = pd.concat([X_te[num_features], encoded_df_te], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SCALING\n",
    "if config.scaler == \"mms\":\n",
    "    mms = MinMaxScaler()\n",
    "    X_tr[num_features] = mms.fit_transform(X_tr[num_features])\n",
    "    X_te[num_features] = mms.transform(X_te[num_features])\n",
    "    \n",
    "elif config.scaler == \"ss\":\n",
    "    ss = StandardScaler()\n",
    "    X_tr[num_features] = ss.fit_transform(X_tr[num_features])\n",
    "    X_te[num_features] = ss.transform(X_te[num_features])\n",
    "    \n",
    "elif config.scaler == \"qt\":\n",
    "    qt = QuantileTransformer(random_state=config.seed, output_distribution='normal', n_quantiles=min(100, len(X_tr) // 5)) # n_quantiles = 1000\n",
    "    \n",
    "    X_tr[num_features] = qt.fit_transform(X_tr[num_features])\n",
    "    X_te[num_features] = qt.transform(X_te[num_features])\n",
    "\n",
    "elif config.scaler == \"pt\":\n",
    "    pts = PowerTransformer(method='yeo-johnson')\n",
    "    \n",
    "    X_tr[num_features] = pts.fit_transform(X_tr[num_features])\n",
    "    X_te[num_features] = pts.transform(X_te[num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################  DOWN SAMPLING  ###############################\n",
    "downsampled_df_tr = resampling.downsample(X_tr, y_tr, method=downsample_options[config.downsampling], random_seed=config.seed)\n",
    "\n",
    "#################################  UP SAMPLING  ###############################\n",
    "cat_idx = [downsampled_df_tr.columns.get_loc(col) for col in cat_features]\n",
    "# cat_idx = [X_tr.columns.get_loc(col) for col in cat_features]\n",
    "X_tr = downsampled_df_tr.drop(\"target\", axis=1)\n",
    "y_tr = downsampled_df_tr[\"target\"]\n",
    "\n",
    "upsampled_df_tr = resampling.upsample(X_tr, y_tr, cat_idx=cat_idx, method=upsample_options[config.upsampling], random_seed=config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RESAMPLED DATA\n",
    "# X_tr = downsampled_df_tr.drop(\"target\", axis=1)\n",
    "# y_tr = downsampled_df_tr[\"target\"]\n",
    "\n",
    "X_tr = upsampled_df_tr.drop(\"target\", axis=1)\n",
    "y_tr = upsampled_df_tr[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################  FEATURE SELECTION  ############################\n",
    "if config.fs_mode:\n",
    "    estimator = classifiers[config.estimator]\n",
    "    estimator.fit(X_tr, y_tr)\n",
    "    \n",
    "    selectors = {\n",
    "        'rfe': RFE(estimator=estimator, n_features_to_select=50),\n",
    "        'sfm': SelectFromModel(estimator=estimator, threshold=\"mean\"),\n",
    "        'kbest': SelectKBest(score_func=f_classif,),\n",
    "    }\n",
    "    \n",
    "    selector = selectors[config.selector]\n",
    "    \n",
    "    X_tr_selec = selector.fit_transform(X_tr, y_tr)\n",
    "    X_te_selec = selector.transform(X_te)\n",
    "    \n",
    "else:\n",
    "    # 기존 열 대신 Bins 열 사용\n",
    "    selected_features = [feature for feature in all_features if feature not in from_bins_features]\n",
    "    \n",
    "    X_tr_selec = X_tr[selected_features]\n",
    "    X_te_selec = X_te[selected_features]\n",
    "    \n",
    "print(\"FEATRUE SELECTION\")\n",
    "print(\"Before \", X_tr.shape)\n",
    "print(\"After \", X_tr_selec.shape, end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################  AutoML  ###########################\n",
    "exp_name = f\"exp_{exp_config}_only_upsample\" \n",
    "\n",
    "classification = setup(data=upsampled_df_tr, target=\"target\",\n",
    "                       session_id=config.seed,\n",
    "                       log_experiment=True, \n",
    "                       experiment_name=exp_name,\n",
    "                       fold=10,\n",
    "                       normalize=True,\n",
    "                       remove_outliers=True,\n",
    "                    #    transformation=True,\n",
    "                       feature_selection=True,\n",
    "                    #    low_variance_threshold = 0.1,\n",
    "                       remove_multicollinearity=True,\n",
    "                       )\n",
    "# feature_selection_method=\"classic\",\n",
    "# n_features_to_select=0.7\n",
    "set_config(\"seed\", config.seed) \n",
    "# normalize_method\n",
    "\n",
    "# catboost = create_model('catboost',fold = 20, return_train_score = True)\n",
    "# tune_catboost =tune_model(catboost, optimize = 'MAE')\n",
    "# dt_results = pull()\n",
    "\n",
    "# sort=\"AUC\" buget_time=0.5\n",
    "# probability_threshold = 0.25\n",
    "# include = ['lr', 'dt', 'lightgbm']\n",
    "best_model_top3 = compare_models(sort='F1', n_select=3)\n",
    "# plot_model(best, plot = 'auc')\n",
    "# plot_model(best, plot = 'confusion_matrix')\n",
    "# results = pull()\n",
    "# blended = blend_models(estimator_list=best_model_top3, fold=5, method='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pull()\n",
    "# ada = create_model(\"ada\")\n",
    "\n",
    "blended_top3 = blend_models(estimator_list=best_model_top3, fold=10, method=\"soft\", weights = [0.5,0.3,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_blended_soft_top3 = tune_model(blended_soft_top3, optimize=\"F1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = finalize_model(tuned_blended_soft_top3)\n",
    "final_preds = predict_model(tuned_blended_soft_top3, data=X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(os.path.join(config.data_path, \"submission.csv\"))\n",
    "df_sub[\"target\"] = final_preds[\"prediction_label\"]\n",
    "df_sub[\"target\"] = df_sub[\"target\"].map({0 : \"Normal\", 1 : \"AbNormal\"})\n",
    "\n",
    "print('=============================')\n",
    "print(df_sub[\"target\"].value_counts())\n",
    "\n",
    "curr_date = datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "\n",
    "# pickle.dump(final_clf, open(f\"{config.model}_{curr_date}.pkl\", \"wb\"))\n",
    "# final_clf = pickle.load(open(\".pkl\", \"rb\"))\n",
    "df_sub.to_csv(os.path.join(config.data_path, f\"submission_{curr_date}_{exp_config}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_holdout = predict_model(blended, data=X_te)\n",
    "# save_model(best, 'my_best_pipeline')\n",
    "# loaded_model = load_model('my_best_pipeline')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
